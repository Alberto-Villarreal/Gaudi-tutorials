{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9599e2f3-6b9c-4578-9501-7d5c65df408a",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d33aa-8f5b-4de3-bdaa-b2bfac88e1e8",
   "metadata": {},
   "source": [
    "# Using Paramater Efficient Fine Tuning on Llama2\n",
    "This example will Fine Tune the Llama2-7B model using Parameter Efficient Fine Tuining (PEFT) and then run inference on a text prompt.  This will be using the Llama2 model with two task examples from the Optimum Habana library on the Hugging Face model repository.   The Optimum Habana library is optimized for Deep Learning training and inference on First-gen Gaudi and Gaudi2 and offers tasks such as text generation, language modeling, question answering and more. For all the examples and models, please refer to the [Optimum Habana GitHub](https://github.com/huggingface/optimum-habana#validated-models).\n",
    "\n",
    "This example will Fine Tune the Llama2-7B model using Parameter Efficient Fine Tuining (PEFT) on the timdettmers/openassistant-guanaco dataset using the Language-Modeling Task in Optimum Habana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337ed16-dcfa-424f-a46d-933880d12d04",
   "metadata": {},
   "source": [
    "### Parameter Efficient Fine Tuning\n",
    "Parameter Efficient Fine Tuning is a strategy for adapting large pre-trained language models to specific tasks while minimizing computational and memory demands.   It aims to reduce the computational cost and memory requirements associated with fine-tuning large models while maintaining or even improving their performance.  It does so by adding a smaller task-specific layer, leveraging knowledge distillation, and often relying on few-shot learning, resulting in efficient yet effective models for various natural language understanding tasks.   PEFT starts with a pre-trained language model that has already learned a wide range of language understanding tasks from a large corpus of text data. These models are usually large and computationally expensive.   Instead of fine-tuning the entire pre-trained model, PEFT adds a task-specific layer or a few task-specific layers on top of the pre-trained model. These additional layers are relatively smaller and have fewer parameters compared to the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404692f3-1266-4dcb-8885-ec6016aa5bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad8bfa-90ae-4e32-963f-821b92ddab0e",
   "metadata": {},
   "source": [
    "### Model Setup: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db7e60-ff31-4a76-9dd5-cb4eb76412b9",
   "metadata": {},
   "source": [
    "##### Install the latest versoin of the Habana Deepspeed Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaab9f1b-f081-4156-b86f-b1e9b23f9a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/HabanaAI/DeepSpeed.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240274a-dd9a-4ca9-a73d-b4ff956c343d",
   "metadata": {},
   "source": [
    "##### Install the Parameter Efficient Fine Tuning Library methods\n",
    "This is taking the PEFT method from the Hugging Face repository and will be used to help create the PEFT Fine Tuning with the Llama2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d21efb3-978e-4585-915a-4c8a9ba9b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'peft'...\n",
      "remote: Enumerating objects: 4772, done.\u001b[K\n",
      "remote: Counting objects: 100% (1687/1687), done.\u001b[K\n",
      "remote: Compressing objects: 100% (450/450), done.\u001b[K\n",
      "remote: Total 4772 (delta 1412), reused 1342 (delta 1192), pack-reused 3085\u001b[K\n",
      "Receiving objects: 100% (4772/4772), 8.60 MiB | 30.67 MiB/s, done.\n",
      "Resolving deltas: 100% (3107/3107), done.\n",
      "/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/peft\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/peft.git\n",
    "%cd peft\n",
    "!pip install -q .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f0f3e-8bf8-4c1c-adbb-3926b292e5ed",
   "metadata": {},
   "source": [
    "##### Install the Optimum-Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8408759-937d-472a-bd00-e67142a90fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade-strategy eager optimum[habana]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b06c57-0d8c-4d40-85ae-1eea945a3ef9",
   "metadata": {},
   "source": [
    "##### Pull the Hugging Face Examples from GitHub\n",
    "These contain the working Hugging Face Task Examples that have been optimized for Gaudi.  For Fine Tuning, we'll use the language-modeling task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50af3c79-6641-47d7-a440-09eba2bd5765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 5995, done.\u001b[K\n",
      "remote: Counting objects: 100% (3034/3034), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1071/1071), done.\u001b[K\n",
      "remote: Total 5995 (delta 2415), reused 2200 (delta 1919), pack-reused 2961\u001b[K\n",
      "Receiving objects: 100% (5995/5995), 3.03 MiB | 16.33 MiB/s, done.\n",
      "Resolving deltas: 100% (3856/3856), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/optimum-habana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e369a6-ce17-40a2-8a52-d735b7140e09",
   "metadata": {},
   "source": [
    "##### Go to the Language Modeling Task and install the model specific requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441de6d9-2b6f-4cf2-8bfd-664cec1c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/language-modeling/\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed7d45",
   "metadata": {},
   "source": [
    "##### How to access and Use the Llama2 model\n",
    "\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/.\n",
    "Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses.\n",
    "\n",
    "To be able to run gated models like this Llama-2-7b-hf, you need the following: \n",
    "- Have a HuggingFace account\n",
    "- Agree to the terms of use of the model in its model card on the HF Hub\n",
    "- set a read token\n",
    "- Login to your account using the HF CLI: run huggingface-cli login before launching your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa53882-c834-4ff3-8fc4-742579ee8cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <your token here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4c81e-c48d-45e0-93de-579e53995602",
   "metadata": {},
   "source": [
    "## Fine Tuning the model with PEFT and LoRA\n",
    "\n",
    "We'll now run the fine tuning with the PEFT method. Remember that the PEFT methods only fine-tune a small number of extra model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning.\n",
    "\n",
    "##### Here's a summary of the command required to run the Fine Tuning, you'll run this in the next cell below. \n",
    "Note in this case the following: \n",
    "1. Using the language modeling with LoRA; `run_lora_clm.py`\n",
    "2. It's very efficient: only 0.06% of the total paramters are being fine tuned of the total 7B parameters.\n",
    "3. Using DeepSpeed has reduced the max amount of memory to 31.03 GB out of a total memory available 94.61 GB\n",
    "4. Only 2 epochs are needed for fine tuning, it takes less than 6 minutes to run. \n",
    "\n",
    "```\n",
    "python ../gaudi_spawn.py --use_deepspeed \\\n",
    "       --world_size 8 run_lora_clm.py \\\n",
    "       --model_name_or_path meta-llama/Llama-2-7b-hf  \\\n",
    "       --dataset_name timdettmers/openassistant-guanaco \\\n",
    "       --bf16 True \\\n",
    "       --output_dir ./model_lora_llama \\\n",
    "       --num_train_epochs 2 \\\n",
    "       --per_device_train_batch_size 2 \\\n",
    "       --per_device_eval_batch_size 2 \\\n",
    "       --gradient_accumulation_steps 4 \\\n",
    "       --evaluation_strategy \"no\"\\\n",
    "       --save_strategy \"steps\"\\\n",
    "       --save_steps 2000 \\\n",
    "       --save_total_limit 1 \\\n",
    "       --learning_rate 1e-4 \\\n",
    "       --logging_steps 1 \\\n",
    "       --dataset_concatenation \\\n",
    "       --do_train \\\n",
    "       --use_habana \\\n",
    "       --use_lazy_mode \\\n",
    "       --throughput_warmup_steps 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6d717b-388b-410a-8983-f3e0fbf3d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2023-10-10 03:19:00,239] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:01,352 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:02,479 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank run_lora_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf --dataset_name timdettmers/openassistant-guanaco --bf16 True --output_dir ./model_lora_llama --num_train_epochs 2 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 2000 --save_total_limit 1 --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3\n",
      "[2023-10-10 03:19:03,911] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-10 03:19:04,170] [WARNING] [runner.py:205:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "Namespace(autotuning='', bind_core_list=None, bind_cores_to_rank=False, elastic_training=False, enable_each_rank_log='None', exclude='', force_multi=False, hostfile='/job/hostfile', include='', launcher='pdsh', launcher_args='', master_addr='', master_port=29500, max_elastic_nodes=-1, min_elastic_nodes=-1, module=False, no_local_rank=True, no_python=False, no_ssh_check=False, num_gpus=8, num_nodes=1, save_pid=False, use_hpu=True, user_args=['--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--dataset_name', 'timdettmers/openassistant-guanaco', '--bf16', 'True', '--output_dir', './model_lora_llama', '--num_train_epochs', '2', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '1e-4', '--logging_steps', '1', '--dataset_concatenation', '--do_train', '--use_habana', '--use_lazy_mode', '--throughput_warmup_steps', '3'], user_script='run_lora_clm.py')\n",
      "[2023-10-10 03:19:06,111] [INFO] [runner.py:583:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None run_lora_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf --dataset_name timdettmers/openassistant-guanaco --bf16 True --output_dir ./model_lora_llama --num_train_epochs 2 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 2000 --save_total_limit 1 --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3\n",
      "[2023-10-10 03:19:08,239] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-10 03:19:08,658] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2023-10-10 03:19:08,658] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2023-10-10 03:19:08,658] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2023-10-10 03:19:08,658] [INFO] [launch.py:164:main] dist_world_size=8\n",
      "[2023-10-10 03:19:08,658] [INFO] [launch.py:166:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2023-10-10 03:19:12,234] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "[2023-10-10 03:19:12,274] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2023-10-10 03:19:12,616] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-10 03:19:12,631] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-10 03:19:12,717] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-10 03:19:12,729] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-10 03:19:12,818] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-10 03:19:12,845] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:13,246 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:13,291 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:13,656 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:13,664 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:13,679 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:13,693 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:13,768 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:177] 2023-10-10 03:19:13,908 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:15,028 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:15,055 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:15,184 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:15,356 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:15,589 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:15,830 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:16,157 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:190] 2023-10-10 03:19:16,168 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "10/10/2023 03:19:19 - WARNING - __main__ -   Process rank: 6, device: hpu:6\n",
      "distributed training: True, 16-bits training: True\n",
      "10/10/2023 03:19:19 - WARNING - __main__ -   Process rank: 3, device: hpu:3\n",
      "distributed training: True, 16-bits training: True\n",
      "10/10/2023 03:19:19 - WARNING - __main__ -   Process rank: 0, device: hpu:0\n",
      "distributed training: True, 16-bits training: True\n",
      "10/10/2023 03:19:19 - WARNING - __main__ -   Process rank: 4, device: hpu:4\n",
      "distributed training: True, 16-bits training: True\n",
      "10/10/2023 03:19:19 - WARNING - __main__ -   Process rank: 7, device: hpu:7\n",
      "distributed training: True, 16-bits training: True\n",
      "10/10/2023 03:19:19 - WARNING - __main__ -   Process rank: 5, device: hpu:5\n",
      "distributed training: True, 16-bits training: True\n",
      "10/10/2023 03:19:19 - WARNING - __main__ -   Process rank: 1, device: hpu:1\n",
      "distributed training: True, 16-bits training: True\n",
      "10/10/2023 03:19:19 - INFO - __main__ -   Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./model_lora_llama/runs/Oct10_03-19-15_sc09super17-klb2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./model_lora_llama,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./model_lora_llama,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "throughput_warmup_steps=3,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "10/10/2023 03:19:19 - WARNING - __main__ -   Process rank: 2, device: hpu:2\n",
      "distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:715] 2023-10-10 03:19:19,250 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json\n",
      "[INFO|configuration_utils.py:775] 2023-10-10 03:19:19,257 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2043] 2023-10-10 03:19:19,521 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2043] 2023-10-10 03:19:19,521 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2043] 2023-10-10 03:19:19,521 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2043] 2023-10-10 03:19:19,521 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2043] 2023-10-10 03:19:19,521 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/tokenizer_config.json\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/10/2023 03:19:20 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/10/2023 03:19:20 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/10/2023 03:19:20 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/10/2023 03:19:20 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/10/2023 03:19:20 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/10/2023 03:19:20 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/10/2023 03:19:20 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/10/2023 03:19:20 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "[INFO|modeling_utils.py:2993] 2023-10-10 03:19:21,349 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1220] 2023-10-10 03:19:21,350 >> Instantiating GaudiLlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:770] 2023-10-10 03:19:21,351 >> Generate config GaudiGenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ignore_eos\": null,\n",
      "  \"static_shapes\": null\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.40s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.63s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.87s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.51s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.52s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.22s/it]\n",
      "[INFO|modeling_utils.py:3775] 2023-10-10 03:20:25,110 >> All model checkpoint weights were used when initializing GaudiLlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3783] 2023-10-10 03:20:25,111 >> All the weights of GaudiLlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GaudiLlamaForCausalLM for predictions without further training.\n",
      "Loading checkpoint shards:  50%|█████████         | 1/2 [00:00<00:00,  1.21it/s][INFO|configuration_utils.py:730] 2023-10-10 03:20:25,209 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/generation_config.json\n",
      "[INFO|configuration_utils.py:770] 2023-10-10 03:20:25,210 >> Generate config GaudiGenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ignore_eos\": null,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"static_shapes\": null,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.78it/s]\n",
      "10/10/2023 03:20:30 - INFO - __main__ -   Using data collator of type DataCollatorForLanguageModeling\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 0\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056447244 KB\n",
      "------------------------------------------------------------------------------\n",
      "[INFO|trainer.py:680] 2023-10-10 03:20:40,619 >> ***** Running training *****\n",
      "[INFO|trainer.py:681] 2023-10-10 03:20:40,619 >>   Num examples = 6,580\n",
      "[INFO|trainer.py:682] 2023-10-10 03:20:40,619 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:683] 2023-10-10 03:20:40,620 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:686] 2023-10-10 03:20:40,620 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:687] 2023-10-10 03:20:40,620 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:688] 2023-10-10 03:20:40,620 >>   Total optimization steps = 206\n",
      "[INFO|trainer.py:689] 2023-10-10 03:20:40,622 >>   Number of trainable parameters = 4,194,304\n",
      "  0%|                                                   | 0/206 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2023-10-10 03:20:40,633 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.6013, 'learning_rate': 9.951456310679612e-05, 'epoch': 0.01, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 23.22, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4856, 'learning_rate': 9.902912621359223e-05, 'epoch': 0.02, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.66, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5767, 'learning_rate': 9.854368932038835e-05, 'epoch': 0.03, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5493, 'learning_rate': 9.805825242718448e-05, 'epoch': 0.04, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5253, 'learning_rate': 9.757281553398059e-05, 'epoch': 0.05, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5641, 'learning_rate': 9.70873786407767e-05, 'epoch': 0.06, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.557, 'learning_rate': 9.660194174757282e-05, 'epoch': 0.07, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5055, 'learning_rate': 9.611650485436893e-05, 'epoch': 0.08, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.603, 'learning_rate': 9.563106796116505e-05, 'epoch': 0.09, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5834, 'learning_rate': 9.514563106796118e-05, 'epoch': 0.1, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5561, 'learning_rate': 9.466019417475729e-05, 'epoch': 0.11, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5545, 'learning_rate': 9.417475728155341e-05, 'epoch': 0.12, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.6204, 'learning_rate': 9.368932038834952e-05, 'epoch': 0.13, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.495, 'learning_rate': 9.320388349514564e-05, 'epoch': 0.14, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5269, 'learning_rate': 9.271844660194175e-05, 'epoch': 0.15, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.531, 'learning_rate': 9.223300970873788e-05, 'epoch': 0.16, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.572, 'learning_rate': 9.174757281553399e-05, 'epoch': 0.17, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5577, 'learning_rate': 9.126213592233011e-05, 'epoch': 0.17, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4731, 'learning_rate': 9.077669902912622e-05, 'epoch': 0.18, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 29.67, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4936, 'learning_rate': 9.029126213592234e-05, 'epoch': 0.19, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.6307, 'learning_rate': 8.980582524271845e-05, 'epoch': 0.2, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5059, 'learning_rate': 8.932038834951457e-05, 'epoch': 0.21, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.518, 'learning_rate': 8.88349514563107e-05, 'epoch': 0.22, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4462, 'learning_rate': 8.834951456310681e-05, 'epoch': 0.23, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4848, 'learning_rate': 8.786407766990292e-05, 'epoch': 0.24, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4802, 'learning_rate': 8.737864077669902e-05, 'epoch': 0.25, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4697, 'learning_rate': 8.689320388349514e-05, 'epoch': 0.26, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5066, 'learning_rate': 8.640776699029127e-05, 'epoch': 0.27, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4813, 'learning_rate': 8.592233009708738e-05, 'epoch': 0.28, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4874, 'learning_rate': 8.54368932038835e-05, 'epoch': 0.29, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4631, 'learning_rate': 8.495145631067961e-05, 'epoch': 0.3, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4053, 'learning_rate': 8.446601941747573e-05, 'epoch': 0.31, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4305, 'learning_rate': 8.398058252427184e-05, 'epoch': 0.32, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.483, 'learning_rate': 8.349514563106797e-05, 'epoch': 0.33, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4102, 'learning_rate': 8.300970873786408e-05, 'epoch': 0.34, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3655, 'learning_rate': 8.25242718446602e-05, 'epoch': 0.35, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.478, 'learning_rate': 8.203883495145631e-05, 'epoch': 0.36, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.5197, 'learning_rate': 8.155339805825243e-05, 'epoch': 0.37, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4851, 'learning_rate': 8.106796116504854e-05, 'epoch': 0.38, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4073, 'learning_rate': 8.058252427184466e-05, 'epoch': 0.39, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4251, 'learning_rate': 8.009708737864078e-05, 'epoch': 0.4, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3586, 'learning_rate': 7.96116504854369e-05, 'epoch': 0.41, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.396, 'learning_rate': 7.912621359223301e-05, 'epoch': 0.42, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4445, 'learning_rate': 7.864077669902913e-05, 'epoch': 0.43, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3733, 'learning_rate': 7.815533980582524e-05, 'epoch': 0.44, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3858, 'learning_rate': 7.766990291262136e-05, 'epoch': 0.45, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3701, 'learning_rate': 7.718446601941748e-05, 'epoch': 0.46, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3982, 'learning_rate': 7.66990291262136e-05, 'epoch': 0.47, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3931, 'learning_rate': 7.621359223300971e-05, 'epoch': 0.48, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4144, 'learning_rate': 7.572815533980583e-05, 'epoch': 0.49, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4168, 'learning_rate': 7.524271844660194e-05, 'epoch': 0.5, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4168, 'learning_rate': 7.475728155339806e-05, 'epoch': 0.5, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3276, 'learning_rate': 7.427184466019417e-05, 'epoch': 0.51, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3983, 'learning_rate': 7.37864077669903e-05, 'epoch': 0.52, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4031, 'learning_rate': 7.330097087378641e-05, 'epoch': 0.53, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3386, 'learning_rate': 7.281553398058253e-05, 'epoch': 0.54, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3929, 'learning_rate': 7.233009708737864e-05, 'epoch': 0.55, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3702, 'learning_rate': 7.184466019417476e-05, 'epoch': 0.56, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3619, 'learning_rate': 7.135922330097087e-05, 'epoch': 0.57, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3886, 'learning_rate': 7.0873786407767e-05, 'epoch': 0.58, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3003, 'learning_rate': 7.038834951456312e-05, 'epoch': 0.59, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3627, 'learning_rate': 6.990291262135923e-05, 'epoch': 0.6, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4152, 'learning_rate': 6.941747572815534e-05, 'epoch': 0.61, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4202, 'learning_rate': 6.893203883495146e-05, 'epoch': 0.62, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3729, 'learning_rate': 6.844660194174757e-05, 'epoch': 0.63, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3319, 'learning_rate': 6.79611650485437e-05, 'epoch': 0.64, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4173, 'learning_rate': 6.747572815533982e-05, 'epoch': 0.65, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3822, 'learning_rate': 6.699029126213593e-05, 'epoch': 0.66, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3609, 'learning_rate': 6.650485436893205e-05, 'epoch': 0.67, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3673, 'learning_rate': 6.601941747572816e-05, 'epoch': 0.68, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4834, 'learning_rate': 6.553398058252428e-05, 'epoch': 0.69, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3731, 'learning_rate': 6.504854368932039e-05, 'epoch': 0.7, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3635, 'learning_rate': 6.456310679611652e-05, 'epoch': 0.71, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3486, 'learning_rate': 6.407766990291263e-05, 'epoch': 0.72, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3832, 'learning_rate': 6.359223300970875e-05, 'epoch': 0.73, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3512, 'learning_rate': 6.310679611650486e-05, 'epoch': 0.74, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.379, 'learning_rate': 6.262135922330098e-05, 'epoch': 0.75, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4656, 'learning_rate': 6.213592233009709e-05, 'epoch': 0.76, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3317, 'learning_rate': 6.16504854368932e-05, 'epoch': 0.77, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4445, 'learning_rate': 6.116504854368932e-05, 'epoch': 0.78, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4072, 'learning_rate': 6.0679611650485434e-05, 'epoch': 0.79, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3529, 'learning_rate': 6.019417475728155e-05, 'epoch': 0.8, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4286, 'learning_rate': 5.970873786407767e-05, 'epoch': 0.81, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3514, 'learning_rate': 5.9223300970873785e-05, 'epoch': 0.82, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3429, 'learning_rate': 5.87378640776699e-05, 'epoch': 0.83, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.2891, 'learning_rate': 5.825242718446602e-05, 'epoch': 0.83, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.2798, 'learning_rate': 5.7766990291262135e-05, 'epoch': 0.84, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3527, 'learning_rate': 5.728155339805825e-05, 'epoch': 0.85, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.378, 'learning_rate': 5.679611650485437e-05, 'epoch': 0.86, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4329, 'learning_rate': 5.6310679611650486e-05, 'epoch': 0.87, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3321, 'learning_rate': 5.58252427184466e-05, 'epoch': 0.88, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3109, 'learning_rate': 5.533980582524272e-05, 'epoch': 0.89, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3963, 'learning_rate': 5.4854368932038836e-05, 'epoch': 0.9, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3009, 'learning_rate': 5.436893203883495e-05, 'epoch': 0.91, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3675, 'learning_rate': 5.3883495145631065e-05, 'epoch': 0.92, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.336, 'learning_rate': 5.339805825242719e-05, 'epoch': 0.93, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3284, 'learning_rate': 5.29126213592233e-05, 'epoch': 0.94, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3579, 'learning_rate': 5.2427184466019416e-05, 'epoch': 0.95, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3048, 'learning_rate': 5.194174757281554e-05, 'epoch': 0.96, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3676, 'learning_rate': 5.145631067961165e-05, 'epoch': 0.97, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4368, 'learning_rate': 5.0970873786407766e-05, 'epoch': 0.98, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3054, 'learning_rate': 5.048543689320389e-05, 'epoch': 0.99, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4229, 'learning_rate': 5e-05, 'epoch': 1.0, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3668, 'learning_rate': 4.951456310679612e-05, 'epoch': 1.01, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4002, 'learning_rate': 4.902912621359224e-05, 'epoch': 1.02, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3025, 'learning_rate': 4.854368932038835e-05, 'epoch': 1.03, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3303, 'learning_rate': 4.805825242718447e-05, 'epoch': 1.04, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3708, 'learning_rate': 4.757281553398059e-05, 'epoch': 1.05, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4266, 'learning_rate': 4.7087378640776703e-05, 'epoch': 1.06, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3861, 'learning_rate': 4.660194174757282e-05, 'epoch': 1.07, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3574, 'learning_rate': 4.611650485436894e-05, 'epoch': 1.08, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3327, 'learning_rate': 4.5631067961165054e-05, 'epoch': 1.09, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3855, 'learning_rate': 4.514563106796117e-05, 'epoch': 1.1, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3392, 'learning_rate': 4.466019417475728e-05, 'epoch': 1.11, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3273, 'learning_rate': 4.4174757281553404e-05, 'epoch': 1.12, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3091, 'learning_rate': 4.368932038834951e-05, 'epoch': 1.13, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3498, 'learning_rate': 4.3203883495145634e-05, 'epoch': 1.14, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3624, 'learning_rate': 4.271844660194175e-05, 'epoch': 1.15, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3917, 'learning_rate': 4.223300970873786e-05, 'epoch': 1.16, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3438, 'learning_rate': 4.1747572815533984e-05, 'epoch': 1.17, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3292, 'learning_rate': 4.12621359223301e-05, 'epoch': 1.17, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3919, 'learning_rate': 4.077669902912621e-05, 'epoch': 1.18, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3639, 'learning_rate': 4.029126213592233e-05, 'epoch': 1.19, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3795, 'learning_rate': 3.980582524271845e-05, 'epoch': 1.2, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3471, 'learning_rate': 3.9320388349514564e-05, 'epoch': 1.21, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3525, 'learning_rate': 3.883495145631068e-05, 'epoch': 1.22, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4095, 'learning_rate': 3.83495145631068e-05, 'epoch': 1.23, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3109, 'learning_rate': 3.7864077669902914e-05, 'epoch': 1.24, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3272, 'learning_rate': 3.737864077669903e-05, 'epoch': 1.25, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3097, 'learning_rate': 3.689320388349515e-05, 'epoch': 1.26, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3404, 'learning_rate': 3.6407766990291265e-05, 'epoch': 1.27, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3961, 'learning_rate': 3.592233009708738e-05, 'epoch': 1.28, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.2621, 'learning_rate': 3.54368932038835e-05, 'epoch': 1.29, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3477, 'learning_rate': 3.4951456310679615e-05, 'epoch': 1.3, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3504, 'learning_rate': 3.446601941747573e-05, 'epoch': 1.31, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3003, 'learning_rate': 3.398058252427185e-05, 'epoch': 1.32, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3675, 'learning_rate': 3.3495145631067966e-05, 'epoch': 1.33, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3485, 'learning_rate': 3.300970873786408e-05, 'epoch': 1.34, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.352, 'learning_rate': 3.2524271844660195e-05, 'epoch': 1.35, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3796, 'learning_rate': 3.2038834951456316e-05, 'epoch': 1.36, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.277, 'learning_rate': 3.155339805825243e-05, 'epoch': 1.37, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3344, 'learning_rate': 3.1067961165048545e-05, 'epoch': 1.38, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4181, 'learning_rate': 3.058252427184466e-05, 'epoch': 1.39, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3227, 'learning_rate': 3.0097087378640774e-05, 'epoch': 1.4, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3469, 'learning_rate': 2.9611650485436892e-05, 'epoch': 1.41, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.2843, 'learning_rate': 2.912621359223301e-05, 'epoch': 1.42, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.337, 'learning_rate': 2.8640776699029125e-05, 'epoch': 1.43, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3924, 'learning_rate': 2.8155339805825243e-05, 'epoch': 1.44, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.337, 'learning_rate': 2.766990291262136e-05, 'epoch': 1.45, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3651, 'learning_rate': 2.7184466019417475e-05, 'epoch': 1.46, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3428, 'learning_rate': 2.6699029126213593e-05, 'epoch': 1.47, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.4084, 'learning_rate': 2.6213592233009708e-05, 'epoch': 1.48, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.2975, 'learning_rate': 2.5728155339805826e-05, 'epoch': 1.49, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3515, 'learning_rate': 2.5242718446601944e-05, 'epoch': 1.5, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3826, 'learning_rate': 2.475728155339806e-05, 'epoch': 1.5, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.324, 'learning_rate': 2.4271844660194176e-05, 'epoch': 1.51, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3554, 'learning_rate': 2.3786407766990294e-05, 'epoch': 1.52, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3043, 'learning_rate': 2.330097087378641e-05, 'epoch': 1.53, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3227, 'learning_rate': 2.2815533980582527e-05, 'epoch': 1.54, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.308, 'learning_rate': 2.233009708737864e-05, 'epoch': 1.55, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3887, 'learning_rate': 2.1844660194174756e-05, 'epoch': 1.56, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3434, 'learning_rate': 2.1359223300970874e-05, 'epoch': 1.57, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3839, 'learning_rate': 2.0873786407766992e-05, 'epoch': 1.58, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3777, 'learning_rate': 2.0388349514563107e-05, 'epoch': 1.59, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.375, 'learning_rate': 1.9902912621359225e-05, 'epoch': 1.6, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3172, 'learning_rate': 1.941747572815534e-05, 'epoch': 1.61, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3443, 'learning_rate': 1.8932038834951457e-05, 'epoch': 1.62, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.34, 'learning_rate': 1.8446601941747575e-05, 'epoch': 1.63, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.298, 'learning_rate': 1.796116504854369e-05, 'epoch': 1.64, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3428, 'learning_rate': 1.7475728155339808e-05, 'epoch': 1.65, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3216, 'learning_rate': 1.6990291262135926e-05, 'epoch': 1.66, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3083, 'learning_rate': 1.650485436893204e-05, 'epoch': 1.67, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3829, 'learning_rate': 1.6019417475728158e-05, 'epoch': 1.68, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3082, 'learning_rate': 1.5533980582524273e-05, 'epoch': 1.69, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3016, 'learning_rate': 1.5048543689320387e-05, 'epoch': 1.7, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3391, 'learning_rate': 1.4563106796116505e-05, 'epoch': 1.71, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.334, 'learning_rate': 1.4077669902912621e-05, 'epoch': 1.72, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3757, 'learning_rate': 1.3592233009708738e-05, 'epoch': 1.73, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.2996, 'learning_rate': 1.3106796116504854e-05, 'epoch': 1.74, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3017, 'learning_rate': 1.2621359223300972e-05, 'epoch': 1.75, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3115, 'learning_rate': 1.2135922330097088e-05, 'epoch': 1.76, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.2652, 'learning_rate': 1.1650485436893204e-05, 'epoch': 1.77, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3782, 'learning_rate': 1.116504854368932e-05, 'epoch': 1.78, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3491, 'learning_rate': 1.0679611650485437e-05, 'epoch': 1.79, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3178, 'learning_rate': 1.0194174757281553e-05, 'epoch': 1.8, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3707, 'learning_rate': 9.70873786407767e-06, 'epoch': 1.81, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3195, 'learning_rate': 9.223300970873788e-06, 'epoch': 1.82, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3756, 'learning_rate': 8.737864077669904e-06, 'epoch': 1.83, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3304, 'learning_rate': 8.25242718446602e-06, 'epoch': 1.83, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3626, 'learning_rate': 7.766990291262136e-06, 'epoch': 1.84, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3714, 'learning_rate': 7.281553398058253e-06, 'epoch': 1.85, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3026, 'learning_rate': 6.796116504854369e-06, 'epoch': 1.86, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3754, 'learning_rate': 6.310679611650486e-06, 'epoch': 1.87, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3138, 'learning_rate': 5.825242718446602e-06, 'epoch': 1.88, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3434, 'learning_rate': 5.3398058252427185e-06, 'epoch': 1.89, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.37, 'learning_rate': 4.854368932038835e-06, 'epoch': 1.9, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3124, 'learning_rate': 4.368932038834952e-06, 'epoch': 1.91, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.368, 'learning_rate': 3.883495145631068e-06, 'epoch': 1.92, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3074, 'learning_rate': 3.3980582524271844e-06, 'epoch': 1.93, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3004, 'learning_rate': 2.912621359223301e-06, 'epoch': 1.94, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3569, 'learning_rate': 2.4271844660194174e-06, 'epoch': 1.95, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.2875, 'learning_rate': 1.941747572815534e-06, 'epoch': 1.96, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.389, 'learning_rate': 1.4563106796116506e-06, 'epoch': 1.97, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3565, 'learning_rate': 9.70873786407767e-07, 'epoch': 1.98, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.325, 'learning_rate': 4.854368932038835e-07, 'epoch': 1.99, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "{'loss': 1.3165, 'learning_rate': 0.0, 'epoch': 2.0, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "100%|█████████████████████████████████████████| 206/206 [05:33<00:00,  1.19it/s][INFO|trainer.py:950] 2023-10-10 03:26:14,417 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 333.7956, 'train_samples_per_second': 74.847, 'train_steps_per_second': 1.172, 'train_loss': 1.3855423747914508, 'epoch': 2.0, 'memory_allocated (GB)': 19.64, 'max_memory_allocated (GB)': 31.26, 'total_memory_available (GB)': 94.46}\n",
      "100%|█████████████████████████████████████████| 206/206 [05:33<00:00,  1.62s/it]\n",
      "[2023-10-10 03:26:19,237] [INFO] [launch.py:348:main] Process 15653 exits successfully.\n",
      "[2023-10-10 03:26:19,238] [INFO] [launch.py:348:main] Process 15655 exits successfully.\n",
      "[2023-10-10 03:26:19,238] [INFO] [launch.py:348:main] Process 15649 exits successfully.\n",
      "[2023-10-10 03:26:20,240] [INFO] [launch.py:348:main] Process 15652 exits successfully.\n",
      "[2023-10-10 03:26:20,240] [INFO] [launch.py:348:main] Process 15648 exits successfully.\n",
      "[2023-10-10 03:26:20,241] [INFO] [launch.py:348:main] Process 15651 exits successfully.\n",
      "[2023-10-10 03:26:20,241] [INFO] [launch.py:348:main] Process 15650 exits successfully.\n",
      "[2023-10-10 03:26:20,242] [INFO] [launch.py:348:main] Process 15654 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!python ../gaudi_spawn.py --use_deepspeed --world_size 8 run_lora_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf  --dataset_name timdettmers/openassistant-guanaco --bf16 True --output_dir ./model_lora_llama --num_train_epochs 2 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 4 --evaluation_strategy \"no\" --save_strategy \"steps\" --save_steps 2000 --save_total_limit 1 --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f4aa5-5dc0-4662-bbca-a7a156be37f2",
   "metadata": {},
   "source": [
    "#### LoRA Fine Tuning Completed\n",
    "You will now see a \"model_lora_llama\" folder created which contains the PEFT model `adapter_model.bin` which will be used in the inference example below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5b5cf-e4a4-4d67-888b-e3ddcdff1a5d",
   "metadata": {},
   "source": [
    "## Inference with Llama2\n",
    "\n",
    "We'll now use the Hugging Face `text-generation` task to run inference on the Llama2-7b model; we'll generate text based on an included prompt.  Notice that we've included a path to the PEFT model that we just created.\n",
    "\n",
    "First, well move to the text-generation examples folder and install the requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14841b58-2697-459d-ace5-763d721468f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/text-generation\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/text-generation\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03479984-e872-48df-b106-cb9b1dd445d3",
   "metadata": {},
   "source": [
    "You will see that we are now running inference with the `run_generation.py` task and we are including the PEFT model that we Fine Tuned in the steps above. \n",
    "\n",
    "```\n",
    "python run_generation.py \\\n",
    "   --model_name_or_path meta-llama/Llama-2-7b-hf \\\n",
    "   --batch_size 1 \\\n",
    "   --do_sample\n",
    "   --max_new_tokens 500 \\\n",
    "   --n_iterations 4 \\\n",
    "   --use_kv_cache \\\n",
    "   --use_hpu_graphs \\\n",
    "   --bf16 \\\n",
    "   --prompt \"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\" \\\n",
    "   --peft_model /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling/model_lora_llama/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e32f89a0-4bdc-4ca4-9855-88060b7f140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt for text generation:  I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter a prompt for text generation: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d466f4a2-3607-4e8f-8b2d-1aefcc7d81aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python run_generation.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 1 --do_sample --max_new_tokens 250 --n_iterations 4 --use_kv_cache --use_hpu_graphs --bf16 --prompt \"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\" --peft_model /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling/model_lora_llama/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/10/2023 05:22:41 - INFO - __main__ - Single-device run.\n",
      "10/10/2023 05:22:42 - INFO - numexpr.utils - Note: detected 160 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "10/10/2023 05:22:42 - INFO - numexpr.utils - Note: NumExpr detected 160 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "10/10/2023 05:22:42 - INFO - numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "[2023-10-10 05:22:42,765] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[WARNING|utils.py:177] 2023-10-10 05:22:43,849 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:190] 2023-10-10 05:22:44,994 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 2559.06it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.49it/s]============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056447244 KB\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "10/10/2023 05:23:46 - INFO - __main__ - Args: Namespace(attn_softmax_bf16=False, bad_words=None, batch_size=1, bf16=True, bucket_size=-1, column_name=None, dataset_max_samples=-1, dataset_name=None, device='hpu', do_sample=True, force_words=None, limit_hpu_graphs=False, local_rank=-1, max_input_tokens=0, max_new_tokens=250, model_name_or_path='meta-llama/Llama-2-7b-hf', model_revision='main', n_iterations=4, num_beams=1, num_return_sequences=1, output_dir=None, peft_model='/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling/model_lora_llama/', profiling_steps=0, profiling_warmup_steps=0, prompt=\"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\", reuse_cache=False, seed=27, token=None, trim_logits=False, use_hpu_graphs=True, use_kv_cache=True, warmup=3)\n",
      "10/10/2023 05:23:46 - INFO - __main__ - device: hpu, n_hpu: 1, bf16: True\n",
      "10/10/2023 05:23:46 - INFO - __main__ - Graph compilation...\n",
      "10/10/2023 05:23:57 - INFO - __main__ - Running generate...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input/outputs:\n",
      "input 1: (\"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\",)\n",
      "output 1: (\"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\\nI'm sorry, but I'm not a dog, and I don't know how to plan a surprise birthday party. But I can give you some ideas for fun activities and games that your human might enjoy.\\nHere are some fun activities and games that your human might enjoy:\\nPuzzle games: Your human might enjoy playing puzzle games like jigsaw puzzles or logic puzzles. You could also look for games that involve building something, like a model airplane or a LEGO set.\\nBoard games: Your human might enjoy playing board games like Monopoly, Clue, or Trivial Pursuit. You could also look for games that involve strategy, like Chess or Checkers.\\nCard games: Your human might enjoy playing card games like Uno, Solitaire, or Poker. You could also look for games that involve strategy, like Rummy or Spades.\\nOutdoor games: Your human might enjoy playing games like Frisbee or Hide and Seek. You could also look for games that involve teamwork, like Capture the Flag or Human Knot.\\nDecorations: You could decorate the room with balloons,\",)\n",
      "\n",
      "\n",
      "Stats:\n",
      "----------------------------------------------------------------------\n",
      "Throughput (including tokenization) = 120.89814310704622 tokens/second\n",
      "Memory allocated                    = 12.91 GB\n",
      "Max memory allocated                = 25.94 GB\n",
      "Total memory available              = 94.46 GB\n",
      "Graph compilation duration          = 10.980633861006936 seconds\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = f'python run_generation.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 1 --do_sample --max_new_tokens 250 --n_iterations 4 --use_kv_cache --use_hpu_graphs \\\n",
    "--bf16 --prompt \"{prompt}\" --peft_model /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling/model_lora_llama/'\n",
    "print(cmd)\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269a38c-eeab-49d8-b12a-73599636e445",
   "metadata": {},
   "source": [
    "##### Comparison without PEFT and LoRA\n",
    "In this example, we're simply running the Llama2 7B model **without** including the PEFT fine tuned model, so the you are losing the additional detail that is brought to the model, and the results have signficantly less information and fidelity compared to the last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a7a9804-cf3e-4bd3-adba-b8d39a9d55a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python run_generation.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 1 --do_sample --max_new_tokens 250 --n_iterations 4 --use_kv_cache --use_hpu_graphs --bf16 --prompt \"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/10/2023 05:24:11 - INFO - __main__ - Single-device run.\n",
      "10/10/2023 05:24:12 - INFO - numexpr.utils - Note: detected 160 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "10/10/2023 05:24:12 - INFO - numexpr.utils - Note: NumExpr detected 160 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "10/10/2023 05:24:12 - INFO - numexpr.utils - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "[2023-10-10 05:24:12,659] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[WARNING|utils.py:177] 2023-10-10 05:24:13,742 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:190] 2023-10-10 05:24:14,897 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 23109.11it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.53it/s]============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056447244 KB\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "10/10/2023 05:25:14 - INFO - __main__ - Args: Namespace(attn_softmax_bf16=False, bad_words=None, batch_size=1, bf16=True, bucket_size=-1, column_name=None, dataset_max_samples=-1, dataset_name=None, device='hpu', do_sample=True, force_words=None, limit_hpu_graphs=False, local_rank=-1, max_input_tokens=0, max_new_tokens=250, model_name_or_path='meta-llama/Llama-2-7b-hf', model_revision='main', n_iterations=4, num_beams=1, num_return_sequences=1, output_dir=None, peft_model=None, profiling_steps=0, profiling_warmup_steps=0, prompt=\"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\", reuse_cache=False, seed=27, token=None, trim_logits=False, use_hpu_graphs=True, use_kv_cache=True, warmup=3)\n",
      "10/10/2023 05:25:14 - INFO - __main__ - device: hpu, n_hpu: 1, bf16: True\n",
      "10/10/2023 05:25:14 - INFO - __main__ - Graph compilation...\n",
      "10/10/2023 05:25:24 - INFO - __main__ - Running generate...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input/outputs:\n",
      "input 1: (\"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\",)\n",
      "output 1: (\"I am a dog. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big bone-shaped cake for me to share with my fur friends!\\nI am a cat. Please help me plan a surprise birthday party for my human, including fun activities, games and decorations. And don't forget to order a big cake for me to share with my fur friends! In this video I'll show you how to install and setup your new Dell laptop.\\nIn this video I'll show you how to install and setup your new Dell laptop. This is a step-by-step video that will walk you through the installation process. A few weeks ago, I had a chance to take a quick trip to San Diego. I spent a few days in the city and then a few days in the mountains. It was a great trip and I had a blast.\\nI have always wanted to visit San Diego and I finally had the chance. I spent a few days in the city and then a few days in the mountains. It was a great trip and I had a blast.\\nI have always wanted to visit San Diego and I finally had the chance. I spent a few days in the city and then a few days in the mountains. It was a great trip and I had a blast. I was able to\",)\n",
      "\n",
      "\n",
      "Stats:\n",
      "----------------------------------------------------------------------\n",
      "Throughput (including tokenization) = 127.37561239469889 tokens/second\n",
      "Memory allocated                    = 12.88 GB\n",
      "Max memory allocated                = 25.92 GB\n",
      "Total memory available              = 94.46 GB\n",
      "Graph compilation duration          = 9.857016128997202 seconds\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = f'python run_generation.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 1 --do_sample --max_new_tokens 250 --n_iterations 4 --use_kv_cache --use_hpu_graphs \\\n",
    "--bf16 --prompt \"{prompt}\"'\n",
    "print(cmd)\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e1747-57ea-44aa-a564-4af8d303d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
