{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b1cdf49",
   "metadata": {},
   "source": [
    "# PyTorch Mixed Precision with Autocast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c12f4da",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.<br>\n",
    "All rights reserved.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the “License”);\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "591a55bf",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it faster and use less memory. By keeping certain parts of the model in the 32-bit types for numerical stability, the model will have a lower step time and train equally as well in terms of the evaluation metrics such as accuracy.\n",
    "\n",
    "Habana HPUs can run operations in bfloat16 faster than float32. Therefore, these lower-precision dtypes should be used whenever possible on HPUs. However, variables and a few computations should still be in float32 for numerical stability so that the model is trained to the same quality. The PyTorch mixed precision allows you to use a mix of bfloat16 and float32 during model training, to get the performance benefits from bfloat16 and the numerical stability benefits from float32.\n",
    "\n",
    "**Autocast is a native PyTorch module that allows running mixed precision training without extensive modifications to the existing FP32 model script. It executes operations registered to autocast using lower precision floating datatype. The module is provided using the torch.amp package.**   For more details on PyTorch autocast, see https://pytorch.org/docs/stable/amp.html.  \n",
    "\n",
    "This simple example shows the basic steps needed to add torch.autocast to a first-gen Gaudi or Gaudi2 based model.  For more details you can refer to the Mixed Precsion documenation or review the Pytorch ResNet50 model example in our Model-References "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52b99b46",
   "metadata": {},
   "source": [
    "## Supported hardware\n",
    "Habana Gaudi HPUs supports a mix of bfloat16 and float32. \n",
    "\n",
    "Even on CPUs, where no speedup is expected, mixed precision APIs can still be used for unit testing, debugging, or just to try out the API. However, on CPUs, mixed precision will run significantly slower."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20c379d7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb26b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52264b70",
   "metadata": {},
   "source": [
    "Set the basic import commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e00be2-c9b5-4fb8-a72c-2b85a645c989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import habana_frameworks.torch.core as htcore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d34fda0-c6a2-49c4-96ee-73c781ca4e21",
   "metadata": {},
   "source": [
    "  \n",
    "In this simple model, you set set the input paramaters, the Linear model, and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd703cd9-8a47-425a-a69f-4a04d5b82e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_LAYOUT_HANDLING = 1\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      " PT_HPU_EAGER_OPS = 0\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 12\n",
      "CPU RAM = 270405564 KB \n",
      "============================================================================================ \n"
     ]
    }
   ],
   "source": [
    "N, D_in, D_out = 64, 1024, 512\n",
    "x = torch.randn(N, D_in, device='hpu')\n",
    "y = torch.randn(N, D_out, device='hpu')\n",
    "\n",
    "model = torch.nn.Linear(D_in, D_out).to(torch.device('hpu'))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5963faf3-3468-4c61-81c2-fc7ad50c6c23",
   "metadata": {},
   "source": [
    "#### To use autocast on HPU, wrap the forward pass (model+loss) of the training to `torch.autocast`:\n",
    "\n",
    "##### Registered Operators  \n",
    "There are three types of registration to torch.autocast:  \n",
    "**Lower precision** - These ops run in the lower precision bfloat16 datatype.  \n",
    "**FP32** - These ops run in the higher precision float32 datatype. \n",
    "**Promote**  These ops run in the highest precision datatypes among its inputs.  \n",
    "\n",
    "**NOTE**  Float16 datatype is not supported. Ensure that BFloat16 specific OPs and functions are used in place of Float16; for example, tensor.bfloat16() should be used instead of tensor.half()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b09044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3303, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3302, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3300, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3298, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3296, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3294, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3292, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3291, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3289, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3287, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3285, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3283, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3281, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3279, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3278, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3276, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3274, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3272, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3270, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3269, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3267, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3265, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3263, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3262, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3260, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3258, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3256, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3254, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3253, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3251, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3249, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3247, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3245, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3243, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3241, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3239, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3237, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3236, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3234, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3232, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3230, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3228, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3226, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3225, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3223, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3221, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3219, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3217, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3215, device='hpu:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3214, device='hpu:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#os.environ['LOWER_LIST'] = '/path/to/lower_list.txt'\n",
    "#os.environ['FP32_LIST'] = '/path/to/fp32_list.txt\n",
    "\n",
    "for t in range(50):\n",
    "   with torch.autocast(device_type='hpu', dtype=torch.bfloat16):\n",
    "       y_pred = model(x)\n",
    "       loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "       print(loss)\n",
    "   optimizer.zero_grad()\n",
    "\n",
    "   loss.backward()\n",
    "   htcore.mark_step()\n",
    "\n",
    "   optimizer.step()\n",
    "   htcore.mark_step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53e4b740",
   "metadata": {},
   "source": [
    "### Supported OPS\n",
    "##### The default list of supported ops for each registration type are internally hard-coded. The following provides the default list of registered ops for each type:\n",
    "\n",
    "Lower precision: addmm, batch_norm, bmm, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, dot, dropout, feature_dropout, group_norm, instance_norm, layer_norm, leaky_relu, linear, matmul, mean, mm, mul, mv, softmax, log_softmax\n",
    "\n",
    "FP32: acos, addcdiv, asin, atan2, bilinear, binary_cross_entropy, binary_cross_entropy_with_logits, cdist, cosh, cosine_embedding_loss, cosine_similarity, cross_entropy_loss, dist, div, divide, embedding, embedding_bag, erfinv, exp, expm1, hinge_embedding_loss, huber_loss, kl_div, l1_loss, log, log10, log1p, log2, logsumexp, margin_ranking_loss, mse_loss, multi_margin_loss, multilabel_margin_loss, nll_loss, pdist, poisson_nll_loss, pow, reciprocal, renorm, rsqrt, sinh, smooth_l1_loss, soft_margin_loss, softplus, tan, topk, triplet_margin_loss, truediv, true_divide\n",
    "\n",
    "Promote: add, addcmul, addcdiv, cat, div, exp, mul, pow, sub, iadd, truediv, stack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
