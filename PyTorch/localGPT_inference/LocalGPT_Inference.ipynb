{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646a18d1-97cd-4ffc-8394-3bff4e8bef6e",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dbf1e-14f5-4b1a-a67a-0859d88bf942",
   "metadata": {},
   "source": [
    "# Using LocalGPT on Gaudi2 with the Llama2 model\n",
    "This tutorial will show how to use the [LocalGPT](https://github.com/PromtEngineer/localGPT) open source initiative on the Gaudi2 processor.  LocalGPT allows you to load your own documents and run an interactive chat session with this material.  This allows you to query and summarize your content by loading any .pdf or .txt documents into the `SOURCE DOCUMENTS` folder, running the ingest.py script to tokenize your content and then the run_localGPT.py script to start the interaction.  \n",
    "\n",
    "In this example, we're using the **meta-llama/Llama-2-13b-chat-hf** model as the refrence model that will manage the inference on Gaudi2. \n",
    "\n",
    "To optimize this instantiation of LocalGPT, we have created new content on top of the existing Hugging Face based \"text-generation\" inference task and pipelines, including:\n",
    "\n",
    "1. Using the Hugging Face Optimum Habana Library with the Llama2-13B model, which is optimized on Gaudi2. \n",
    "2. Using Langchain to import the source docuement with a custom embedding model, using a `GaudiHuggingFaceInstructEmbeddings` class based on HuggingFaceInstructEmbeddings.\n",
    "3. We are using a custom pipeline class, `GaudiTextGenerationPipeline` that optimizes text-generation tasks for padding and indexing for static shapes, to improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c604050-d6f5-47a9-adc7-0b9e3c0fe34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/localGPT_inference\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi-tutorials/PyTorch/localGPT_inference\n",
    "!export DEBIAN_FRONTEND=\"noninteractive\"\n",
    "!export TZ=Etc/UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e2383-deaf-4349-90e5-31bea9a507b0",
   "metadata": {},
   "source": [
    "##### Install the requirements for LocalGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadcf84-eb2b-4f04-9cbf-4e6d67d4e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y tzdata bash-completion python3-pip openssh-server      vim git iputils-ping net-tools protobuf-compiler curl bc gawk tmux     && rm -rf /var/lib/apt/lists/*\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d111e-2118-4bb0-b62d-214e895479a2",
   "metadata": {},
   "source": [
    "##### Install the Optimum Habana Library from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217c2833-0bb7-481e-907d-7b28d88bbf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade-strategy eager optimum[habana]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2eaba6-2d4b-4007-bb13-ca5113022aad",
   "metadata": {},
   "source": [
    "### Load your Local Content\n",
    "Copy all of your files into the `SOURCE_DOCUMENTS` directory\n",
    "\n",
    "The current default file types are .txt, .pdf, .csv, and .xlsx, if you want to use any other file type, you will need to convert it to one of the default file types.\n",
    "\n",
    "Run the following command to ingest all the data. The ingest.py uses LangChain tools to parse the document and create embeddings locally using the GaudiHuggingFaceInstructEmbeddings class. It then stores the result in a local vector database (DB) using Chroma vector store. \n",
    "\n",
    "If you want to start from an empty database, delete the DB folder and run the ingest script again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db570f4-acce-4e73-8ac1-7742e621c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 23:23:58,137 - INFO - ingest.py:124 - Loading documents from /root/Gaudi-tutorials/PyTorch/localGPT_inference/SOURCE_DOCUMENTS\n",
      "2023-10-10 23:23:58,148 - INFO - ingest.py:37 - Loading document batch\n",
      "2023-10-10 23:24:48,208 - INFO - ingest.py:133 - Loaded 1 documents from /root/Gaudi-tutorials/PyTorch/localGPT_inference/SOURCE_DOCUMENTS\n",
      "2023-10-10 23:24:48,208 - INFO - ingest.py:134 - Split into 2227 chunks of text\n",
      "Loading Habana modules from /usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/lib\n",
      "2023-10-10 23:24:49,625 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-10-10 23:24:50,149 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-10-10 23:24:50,453 - INFO - __init__.py:88 - Running Chroma using direct local API.\n",
      "2023-10-10 23:24:50,713 - WARNING - __init__.py:43 - Using embedded DuckDB with persistence: data will be stored in: /root/Gaudi-tutorials/PyTorch/localGPT_inference/DB\n",
      "2023-10-10 23:24:50,719 - INFO - ctypes.py:22 - Successfully imported ClickHouse Connect C data optimizations\n",
      "2023-10-10 23:24:50,723 - INFO - json_impl.py:45 - Using python library for writing JSON byte strings\n",
      "2023-10-10 23:24:50,950 - INFO - duckdb.py:460 - loaded in 4454 embeddings\n",
      "2023-10-10 23:24:50,952 - INFO - duckdb.py:472 - loaded in 1 collections\n",
      "2023-10-10 23:24:50,953 - INFO - duckdb.py:89 - collection with name langchain already exists, returning existing collection\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056447244 KB\n",
      "------------------------------------------------------------------------------\n",
      "Batches: 100%|██████████████████████████████████| 70/70 [00:02<00:00, 23.41it/s]\n",
      "2023-10-10 23:24:58,235 - INFO - ingest.py:161 - Time taken to create embeddings vectorstore: 7.784449464001227s\n",
      "2023-10-10 23:24:58,235 - INFO - duckdb.py:414 - Persisting DB to disk, putting it in the save folder: /root/Gaudi-tutorials/PyTorch/localGPT_inference/DB\n",
      "2023-10-10 23:24:58,619 - INFO - duckdb.py:414 - Persisting DB to disk, putting it in the save folder: /root/Gaudi-tutorials/PyTorch/localGPT_inference/DB\n"
     ]
    }
   ],
   "source": [
    "!python ingest.py --device_type hpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03495bec-61c5-4b49-a2ee-387c75ba088e",
   "metadata": {},
   "source": [
    "### How to access and Use the Llama2 model\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses.\n",
    "\n",
    "To be able to run gated models like this Llama-2-13b-chat-hf, you need the following:\n",
    "\n",
    "* Have a HuggingFace account\n",
    "* Agree to the terms of use of the model in its model card on the HF Hub\n",
    "* Set a read token\n",
    "* Login to your account using the HF CLI: run huggingface-cli login before launching your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb59c62-41c4-45b5-a642-3b61b341d135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <your token here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac741b-cb79-4fab-a1aa-efa075503d61",
   "metadata": {},
   "source": [
    "### Running the LocalGPT model with Llama2 13B Chat \n",
    "\n",
    "### Set the model Usage\n",
    "\n",
    "To change the model, you can modify the \"LLM_ID = <add model here>\" in the `constants.py` file. For this example, the default is `meta-llama/Llama-2-13b-chat-hf`\n",
    "\n",
    "Since this is interactive, it's a better experince to launch this from a terminal window.  This run_localGPT.py script uses a local LLM (Llama2 in this case) to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the documentation.\n",
    "\n",
    "`python run_localGPT.py --device_type hpu`\n",
    "\n",
    "Note: The inference is running sampling mode, so the user can optinally modify the temperature and top_p settings in run_localGPT.py line 84 to modify the output.  The current settings are temperature=0.5, top_p=0.8.  Type \"exit\" at the prompt to stop the execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2abdc3-850e-4f55-8dc4-6f90d47bfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a terminal window to start the chat: `python run_localGPT.py --device_type hpu`, the example below is showing the initial output:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc237333-fbdc-440e-8249-89639af4c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 23:29:55,812 - INFO - run_localGPT.py:186 - Running on: hpu\n",
      "2023-10-10 23:29:55,812 - INFO - run_localGPT.py:187 - Display Source Documents set to: False\n",
      "2023-10-10 23:29:56,315 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-10-10 23:29:56,718 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-10-10 23:29:56,922 - INFO - __init__.py:88 - Running Chroma using direct local API.\n",
      "2023-10-10 23:29:56,931 - WARNING - __init__.py:43 - Using embedded DuckDB with persistence: data will be stored in: /root/Gaudi-tutorials/PyTorch/localGPT_inference/DB\n",
      "2023-10-10 23:29:56,935 - INFO - ctypes.py:22 - Successfully imported ClickHouse Connect C data optimizations\n",
      "2023-10-10 23:29:56,938 - INFO - json_impl.py:45 - Using python library for writing JSON byte strings\n",
      "2023-10-10 23:29:57,183 - INFO - duckdb.py:460 - loaded in 6681 embeddings\n",
      "2023-10-10 23:29:57,184 - INFO - duckdb.py:472 - loaded in 1 collections\n",
      "2023-10-10 23:29:57,185 - INFO - duckdb.py:89 - collection with name langchain already exists, returning existing collection\n",
      "2023-10-10 23:29:57,186 - INFO - run_localGPT.py:38 - Loading Model: meta-llama/Llama-2-13b-chat-hf, on: hpu\n",
      "2023-10-10 23:29:57,186 - INFO - run_localGPT.py:39 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2023-10-10 23:29:57,622] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[WARNING|utils.py:177] 2023-10-10 23:29:58,637 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but habana-frameworks v1.12.0.480 was found, this could lead to undefined behavior!\n",
      "[WARNING|utils.py:190] 2023-10-10 23:29:59,786 >> optimum-habana v1.7.5 has been validated for SynapseAI v1.11.0 but the driver version is v1.12.0, this could lead to undefined behavior!\n",
      "Fetching 3 files: 100%|████████████████████████| 3/3 [00:00<00:00, 17427.86it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.12it/s]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056447244 KB\n",
      "------------------------------------------------------------------------------\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "2023-10-10 23:32:20,404 - INFO - run_localGPT.py:133 - Local LLM Loaded\n",
      "\n",
      "Enter a query: ^C\n",
      "Received Interrupt\n"
     ]
    }
   ],
   "source": [
    "python run_localGPT.py --device_type hpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe1723-04da-4d59-a299-fe0431a84bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
