{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646a18d1-97cd-4ffc-8394-3bff4e8bef6e",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dbf1e-14f5-4b1a-a67a-0859d88bf942",
   "metadata": {},
   "source": [
    "# Using LocalGPT on the Intel&reg; Gaudi&reg; 2 AI accelerator with the Llama 2 70B model\n",
    "This tutorial will show how to use the [LocalGPT](https://github.com/PromtEngineer/localGPT) open source initiative on the Intel Gaudi 2 AI accelerator.  LocalGPT allows you to load your own documents and run an interactive chat session with this material.  This allows you to query and summarize your content by loading any .pdf or .txt documents into the `SOURCE DOCUMENTS` folder, using utilities from the ingest.py script to tokenize your content and then the run_localGPT.py script to start the interaction.  \n",
    "\n",
    "In this example, we're using the **meta-llama/Llama-2-70b-chat-hf** model as the reference model that will manage the inference on Gaudi 2.  DeepSpeed inference is used based on the size of the model.\n",
    "\n",
    "To optimize this instantiation of LocalGPT, we have created new content on top of the existing Hugging Face based \"text-generation\" inference task and pipelines, including:\n",
    "\n",
    "1. Using the Hugging Face Optimum Habana Library with the Llama 2 70B model, which is optimized on Gaudi2. \n",
    "2. Using LangChain to import the source document with a custom embedding model, using a `GaudiHuggingFaceEmbeddings` class based on HuggingFaceEmbeddings.\n",
    "3. We are using a custom pipeline class, `GaudiTextGenerationPipeline` that optimizes text-generation tasks for padding and indexing for static shapes, to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eab16b-6912-4676-af1f-91b7472c90d4",
   "metadata": {},
   "source": [
    "##### Install DeepSpeed to run inference on the full Llama 2 70B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d7fd64-18b0-4a71-8484-813defa94062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.14.0\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.14.0) to /tmp/pip-req-build-4h71r15b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-4h71r15b\n",
      "  Running command git checkout -b 1.14.0 --track origin/1.14.0\n",
      "  Switched to a new branch '1.14.0'\n",
      "  Branch '1.14.0' set up to track remote branch '1.14.0' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit fad45b24c7c9070251711a0d7d6f1b82805072ad\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.12.4+hpu.synapse.v1.14.0)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.4+hpu.synapse.v1.14.0) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.4+hpu.synapse.v1.14.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.4+hpu.synapse.v1.14.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.4+hpu.synapse.v1.14.0) (5.9.8)\n",
      "Collecting py-cpuinfo (from deepspeed==0.12.4+hpu.synapse.v1.14.0)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.4+hpu.synapse.v1.14.0) (1.10.13)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.4+hpu.synapse.v1.14.0) (8.0.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.4+hpu.synapse.v1.14.0) (2.1.1a0+gitb51c9f6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.4+hpu.synapse.v1.14.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.12.4+hpu.synapse.v1.14.0) (4.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.12.4+hpu.synapse.v1.14.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.12.4+hpu.synapse.v1.14.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.12.4+hpu.synapse.v1.14.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.12.4+hpu.synapse.v1.14.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.12.4+hpu.synapse.v1.14.0) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.12.4+hpu.synapse.v1.14.0) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.12.4+hpu.synapse.v1.14.0) (1.3.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.12.4+hpu.synapse.v1.14.0-py3-none-any.whl size=1322511 sha256=24bf9ea65ae78e43b40b8af26236dda338949baa3ae50a0ef75ba93292edaf11\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xc8f_vk6/wheels/41/e1/8c/d69b0d3f45265e581b36c00ad72fbe5595c8c614d66c240171\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, hjson, deepspeed\n",
      "Successfully installed deepspeed-0.12.4+hpu.synapse.v1.14.0 hjson-3.1.0 py-cpuinfo-9.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1b812-5032-4a26-867e-155575a14991",
   "metadata": {},
   "source": [
    "##### Go to the LocalGPT folder and set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c604050-d6f5-47a9-adc7-0b9e3c0fe34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/localGPT_inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi-tutorials/PyTorch/localGPT_inference\n",
    "!export DEBIAN_FRONTEND=\"noninteractive\"\n",
    "!export TZ=Etc/UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e2383-deaf-4349-90e5-31bea9a507b0",
   "metadata": {},
   "source": [
    "##### Install the requirements for LocalGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deadcf84-eb2b-4f04-9cbf-4e6d67d4e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,455 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,065 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,784 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1,792 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,339 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,735 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,822 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\n",
      "Fetched 29.7 MB in 4s (8,299 kB/s)                          \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "bc is already the newest version (1.07.1-3build1).\n",
      "curl is already the newest version (7.81.0-1ubuntu1.15).\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
      "openssh-server is already the newest version (1:8.9p1-3ubuntu0.6).\n",
      "tzdata is already the newest version (2023d-0ubuntu0.22.04).\n",
      "tzdata set to manually installed.\n",
      "vim is already the newest version (2:8.2.3995-1ubuntu2.15).\n",
      "protobuf-compiler is already the newest version (3.12.4-1ubuntu7.22.04.1).\n",
      "python3-pip is already the newest version (22.0.2+dfsg-1ubuntu0.4).\n",
      "The following additional packages will be installed:\n",
      "  libsigsegv2 libutempter0\n",
      "Suggested packages:\n",
      "  gawk-doc\n",
      "The following NEW packages will be installed:\n",
      "  bash-completion gawk iputils-ping libsigsegv2 libutempter0 net-tools tmux\n",
      "0 upgraded, 7 newly installed, 0 to remove and 13 not upgraded.\n",
      "Need to get 1,326 kB of archives.\n",
      "After this operation, 5,307 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsigsegv2 amd64 2.13-1ubuntu3 [14.6 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gawk amd64 1:5.1.0-1ubuntu0.1 [447 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 iputils-ping amd64 3:20211215-1 [42.9 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 bash-completion all 1:2.11-5ubuntu1 [180 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libutempter0 amd64 1.2.1-2build2 [8,848 B]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 net-tools amd64 1.60+git20181103.0eebece-1ubuntu5 [204 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 tmux amd64 3.2a-4ubuntu0.2 [428 kB]\n",
      "Fetched 1,326 kB in 1s (1,818 kB/s)\n",
      "Selecting previously unselected package libsigsegv2:amd64.\n",
      "(Reading database ... 49978 files and directories currently installed.)\n",
      "Preparing to unpack .../libsigsegv2_2.13-1ubuntu3_amd64.deb ...\n",
      "Unpacking libsigsegv2:amd64 (2.13-1ubuntu3) ...\n",
      "Setting up libsigsegv2:amd64 (2.13-1ubuntu3) ...\n",
      "Selecting previously unselected package gawk.\n",
      "(Reading database ... 49985 files and directories currently installed.)\n",
      "Preparing to unpack .../0-gawk_1%3a5.1.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking gawk (1:5.1.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package iputils-ping.\n",
      "Preparing to unpack .../1-iputils-ping_3%3a20211215-1_amd64.deb ...\n",
      "Unpacking iputils-ping (3:20211215-1) ...\n",
      "Selecting previously unselected package bash-completion.\n",
      "Preparing to unpack .../2-bash-completion_1%3a2.11-5ubuntu1_all.deb ...\n",
      "Unpacking bash-completion (1:2.11-5ubuntu1) ...\n",
      "Selecting previously unselected package libutempter0:amd64.\n",
      "Preparing to unpack .../3-libutempter0_1.2.1-2build2_amd64.deb ...\n",
      "Unpacking libutempter0:amd64 (1.2.1-2build2) ...\n",
      "Selecting previously unselected package net-tools.\n",
      "Preparing to unpack .../4-net-tools_1.60+git20181103.0eebece-1ubuntu5_amd64.deb ...\n",
      "Unpacking net-tools (1.60+git20181103.0eebece-1ubuntu5) ...\n",
      "Selecting previously unselected package tmux.\n",
      "Preparing to unpack .../5-tmux_3.2a-4ubuntu0.2_amd64.deb ...\n",
      "Unpacking tmux (3.2a-4ubuntu0.2) ...\n",
      "Setting up net-tools (1.60+git20181103.0eebece-1ubuntu5) ...\n",
      "Setting up gawk (1:5.1.0-1ubuntu0.1) ...\n",
      "Setting up bash-completion (1:2.11-5ubuntu1) ...\n",
      "Setting up libutempter0:amd64 (1.2.1-2build2) ...\n",
      "Setting up iputils-ping (3:20211215-1) ...\n",
      "Setting up tmux (3.2a-4ubuntu0.2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.6) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y tzdata bash-completion python3-pip openssh-server      vim git iputils-ping net-tools protobuf-compiler curl bc gawk tmux     && rm -rf /var/lib/apt/lists/*\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d111e-2118-4bb0-b62d-214e895479a2",
   "metadata": {},
   "source": [
    "##### Install the Optimum Habana Library from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217c2833-0bb7-481e-907d-7b28d88bbf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q optimum-habana==1.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095322a0",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "LocalGPT uses Retrieval-Augmented Generation (RAG) at it's core. RAG is a relatively new AI technique that combines an information retrieval system with text-generation models/LLMs. It provides an effective way to ground LLMs by using retrieved contexts from an external knowledge base, without having to perform retraining or finetuning.\n",
    "The LocalGPT application workflow can be broken down as follows:\n",
    "* Document Ingestion: This step involves creating an external knowledge base via a vector database. The text present in the documents is parsed, split into chunks and converted to embeddings using an embedding model. The vector embeddings are finally stored in the vector database.\n",
    "\n",
    "![](img/ingest.jpg)\n",
    "\n",
    "* Text Generation: This step involves accepting a query from the user, converting the query to embeddings and retrieving appropriate contexts from the knowledge base. The input prompt to the LLM is the concatenation of the query, contexts and chat history.\n",
    "\n",
    "![](img/documentqa.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2eaba6-2d4b-4007-bb13-ca5113022aad",
   "metadata": {},
   "source": [
    "### Document Ingestion\n",
    "Copy all of your files into the `SOURCE_DOCUMENTS` directory\n",
    "\n",
    "The current default file types are .txt, .pdf, .csv, and .xlsx, if you want to use any other file type, you will need to convert it to one of the default file types.\n",
    "\n",
    "Run the following cells to ingest all the data. This notebook uses LangChain tools to parse the documents and create embeddings locally using the GaudiHuggingFaceEmbeddings class. It then stores the result in a local vector database (DB) using Chroma vector store. \n",
    "\n",
    "If you want to start from an empty database, delete the DB folder and run the next few cells again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ae917-86df-4061-9f9a-d6cb2bb7cd75",
   "metadata": {},
   "source": [
    "##### Load your files as LangChain Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db570f4-acce-4e73-8ac1-7742e621c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from /root/Gaudi-tutorials/PyTorch/localGPT_inference/SOURCE_DOCUMENTS\n"
     ]
    }
   ],
   "source": [
    "from constants import SOURCE_DIRECTORY\n",
    "from ingest import load_documents\n",
    "\n",
    "documents = load_documents(SOURCE_DIRECTORY)\n",
    "print(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730b93b-eb2c-45a9-9989-203b7de37be4",
   "metadata": {},
   "source": [
    "##### Split the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a90c469-dcae-49aa-8c80-a3043072c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 72 chunks of text\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(texts)} chunks of text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ab0c9-f379-4870-a36e-6f221a6520a1",
   "metadata": {},
   "source": [
    "##### Create embeddings from chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ebe1ee-f066-4083-aa53-eddb73996be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Habana modules from /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading .gitattributes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 1.71MB/s]\n",
      "Downloading 1_Pooling/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 190/190 [00:00<00:00, 161kB/s]\n",
      "Downloading README.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10.7k/10.7k [00:00<00:00, 7.87MB/s]\n",
      "Downloading config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 612/612 [00:00<00:00, 1.34MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 255kB/s]\n",
      "Downloading data_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 39.3k/39.3k [00:00<00:00, 734kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 90.9M/90.9M [00:00<00:00, 111MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 116kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 235kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 43.5MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 350/350 [00:00<00:00, 275kB/s]\n",
      "Downloading train_script.py: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 13.2k/13.2k [00:00<00:00, 22.8MB/s]\n",
      "Downloading vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 1.91MB/s]\n",
      "Downloading modules.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 768kB/s]\n"
     ]
    }
   ],
   "source": [
    "from constants import EMBEDDING_INPUT_SIZE, EMBEDDING_MODEL_NAME\n",
    "from gaudi_utils.embeddings import GaudiHuggingFaceEmbeddings\n",
    "\n",
    "from habana_frameworks.torch.utils.library_loader import load_habana_module\n",
    "load_habana_module()\n",
    "\n",
    "embeddings = GaudiHuggingFaceEmbeddings(embedding_input_size=EMBEDDING_INPUT_SIZE, model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"hpu\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079b506-0fcf-49ca-9865-0ebfa8170df3",
   "metadata": {},
   "source": [
    "##### Create a Chroma vector database to store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68b30209-a897-40b5-aaab-fe0c33585c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create vector store: 250.5448590964079 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from constants import PERSIST_DIRECTORY, CHROMA_SETTINGS\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=PERSIST_DIRECTORY, client_settings=CHROMA_SETTINGS)\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time taken to create vector store: {(end_time-start_time)*1000} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01705607",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "Once the Chroma vector database is ready, we can explore the text-generation component of LocalGPT.\n",
    "\n",
    "The next few cells describe all the steps in the text generation process. We use the smallest Llama 2 model **meta-llama/Llama-2-7b-chat-hf** to perform augmented text-generation after retrieving relevant contexts from the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d90d69",
   "metadata": {},
   "source": [
    "##### Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_localGPT import load_model\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "llm, _ = load_model(device_type=\"hpu\", model_id=model_id, temperature=0.2, top_p=0.95, model_basename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d001a99",
   "metadata": {},
   "source": [
    "##### Define the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ff0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b188c",
   "metadata": {},
   "source": [
    "##### Create the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5892a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer,\\\n",
    "just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba70acaf",
   "metadata": {},
   "source": [
    "##### Initialize a LangChain object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97272db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb87269",
   "metadata": {},
   "source": [
    "##### Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb908353",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qa(\"What is this document about?\")\n",
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03495bec-61c5-4b49-a2ee-387c75ba088e",
   "metadata": {},
   "source": [
    "### How to access and Use the Llama 2 model\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses.\n",
    "\n",
    "To be able to run gated models like this Llama-2-70b-chat-hf, you need the following:\n",
    "\n",
    "* Have a HuggingFace account\n",
    "* Agree to the terms of use of the model in its model card on the HF Hub\n",
    "* Set a read token\n",
    "* Login to your account using the HF CLI: run huggingface-cli login before launching your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb59c62-41c4-45b5-a642-3b61b341d135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#!huggingface-cli login --token <your token here>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3ac741b-cb79-4fab-a1aa-efa075503d61",
   "metadata": {},
   "source": [
    "### Running the LocalGPT model with Llama2 70B Chat \n",
    "\n",
    "### Set the model Usage\n",
    "\n",
    "To change the model, you can modify the \"LLM_ID = <add model here>\" in the `constants.py` file. For this example, the default is `meta-llama/Llama-2-70b-chat-hf`.  \n",
    "\n",
    "Since this is interactive, it's a better experince to launch this from a terminal window.  This run_localGPT.py script uses a local LLM (Llama 2 in this case) to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the documentation.  This is the run command to use:\n",
    "\n",
    "`PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`\n",
    "\n",
    "Running the full 70B model takes up ~128GB of disk space, so if your system is storage constrained, it may be best to run the Llama 2 7B or 13B chat models.  Change the LLM_ID variable in the `constants.py` file (example: `LLM_ID = \"meta-llama/Llama-2-7b-chat-hf\"`) and use the command below.\n",
    "`python run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`\n",
    "\n",
    "Note: The inference is running sampling mode, so the user can optinally modify the temperature and top_p settings.  The current settings are temperature=0.7, top_p=0.95.  Type \"exit\" at the prompt to stop the execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee22fd-2bca-49b8-8f8d-501075058b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this command in a terminal window to start the interactive chat: `PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`, the example below is showing the initial output:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc237333-fbdc-440e-8249-89639af4c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank run_localGPT.py --device_type hpu --temperature 0.2 --top_p 0.95\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2023-12-09 01:30:35,163] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:36,512] [WARNING] [runner.py:206:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-12-09 01:30:36,575] [INFO] [runner.py:585:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None run_localGPT.py --device_type hpu --temperature 0.2 --top_p 0.95\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2023-12-09 01:30:38,987] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:164:main] dist_world_size=8\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:166:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "2023-12-09 01:30:43,978 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:43,978 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:43,979 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:43,979 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:43,979 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,118 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,119 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,119 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,119 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,119 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "2023-12-09 01:30:44,330 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,330 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,331 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,331 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,331 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2023-12-09 01:30:47,114] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,195] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,347] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,356] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,446] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,585] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,864] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,874] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:48,389] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,389] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:48,492] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,492] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:48,492] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl\n",
      "[2023-12-09 01:30:48,498] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,498] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:48,588] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,588] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:48,696] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,696] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:49,026] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:49,026] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:49,198] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:49,198] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:49,569] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:49,569] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 76912.67it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 70138.86it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 39028.88it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 29916.58it/s]\n",
      "Fetching 15 files: 100%|██████████████████████| 15/15 [00:00<00:00, 7181.21it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 60436.66it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 22098.55it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 66156.21it/s]\n",
      "Fetching 15 files: 100%|████████████████████| 15/15 [00:00<00:00, 125078.65it/s]\n",
      "[2023-12-09 01:31:14,039] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3+hpu.synapse.v1.13.0, git-hash=6522014, git-branch=1.13.0\n",
      "[2023-12-09 01:31:14,042] [INFO] [logging.py:96:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Loading 15 checkpoint shards:   0%|                      | 0/15 [00:00<?, ?it/s]============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446912 KB\n",
      "------------------------------------------------------------------------------\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:10<00:00,  4.67s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:13<00:00,  4.88s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:19<00:00,  5.29s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:21<00:00,  5.43s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:23<00:00,  5.60s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:25<00:00,  5.69s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:26<00:00,  5.78s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:29<00:00,  5.95s/it]\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,014 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,176 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,178 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,242 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,242 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,242 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,242 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,247 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,255 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,582 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:59,854 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,164 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,541 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,553 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,582 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,638 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,639 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "\n",
      "Enter a query: "
     ]
    }
   ],
   "source": [
    "!PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.2 --top_p 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe1723-04da-4d59-a299-fe0431a84bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba45e02-5a1f-4dd0-863e-5bdf8e2cbca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
