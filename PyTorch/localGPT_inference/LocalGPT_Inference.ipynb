{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646a18d1-97cd-4ffc-8394-3bff4e8bef6e",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dbf1e-14f5-4b1a-a67a-0859d88bf942",
   "metadata": {},
   "source": [
    "# Using LocalGPT on the Intel&reg; Gaudi&reg; 2 AI accelerator with the Llama2 70B model\n",
    "This tutorial will show how to use the [LocalGPT](https://github.com/PromtEngineer/localGPT) open source initiative on the Intel Gaudi 2 AI accelerator.  LocalGPT allows you to load your own documents and run an interactive chat session with this material.  This allows you to query and summarize your content by loading any .pdf or .txt documents into the `SOURCE DOCUMENTS` folder, running the ingest.py script to tokenize your content and then the run_localGPT.py script to start the interaction.  \n",
    "\n",
    "In this example, we're using the **meta-llama/Llama-2-70b-chat-hf** model as the refrence model that will manage the inference on Gaudi 2.  DeepSpeed inference is used based on the size of the model.\n",
    "\n",
    "To optimize this instantiation of LocalGPT, we have created new content on top of the existing Hugging Face based \"text-generation\" inference task and pipelines, including:\n",
    "\n",
    "1. Using the Hugging Face Optimum Habana Library with the Llama2-70B model, which is optimized on Gaudi2. \n",
    "2. Using Langchain to import the source docuement with a custom embedding model, using a `GaudiHuggingFaceEmbeddings` class based on HuggingFaceEmbeddings.\n",
    "3. We are using a custom pipeline class, `GaudiTextGenerationPipeline` that optimizes text-generation tasks for padding and indexing for static shapes, to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eab16b-6912-4676-af1f-91b7472c90d4",
   "metadata": {},
   "source": [
    "##### Install DeepSpeed to run inference on the full Llama2 70B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d7fd64-18b0-4a71-8484-813defa94062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.14.0\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.13.0) to /tmp/pip-req-build-ptlbmyw4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-ptlbmyw4\n",
      "  Running command git checkout -b 1.13.0 --track origin/1.13.0\n",
      "  Switched to a new branch '1.13.0'\n",
      "  Branch '1.13.0' set up to track remote branch '1.13.0' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit 6522014efac08fdcbc37ea4a9d85552ce9cb7b50\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.10.3+hpu.synapse.v1.13.0)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed==0.10.3+hpu.synapse.v1.13.0)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.10.13)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (2.1.0a0+gitf8b6084)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->deepspeed==0.10.3+hpu.synapse.v1.13.0) (4.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.3.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.10.3+hpu.synapse.v1.13.0-py3-none-any.whl size=941404 sha256=d5d3cac4bb0f61e408f53b3c0f1e228bd435a27efa4fe8139075622cff6b77ce\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-insmzxsu/wheels/80/ad/7c/2b51ee0f18d4027d2c8ef64ef28ab3a3943c46d606ab4417f4\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, hjson, deepspeed\n",
      "Successfully installed deepspeed-0.10.3+hpu.synapse.v1.13.0 hjson-3.1.0 py-cpuinfo-9.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1b812-5032-4a26-867e-155575a14991",
   "metadata": {},
   "source": [
    "##### Go to the LocalGPT folder and set environment varialbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c604050-d6f5-47a9-adc7-0b9e3c0fe34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/localGPT_inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi-tutorials/PyTorch/localGPT_inference\n",
    "!export DEBIAN_FRONTEND=\"noninteractive\"\n",
    "!export TZ=Etc/UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e2383-deaf-4349-90e5-31bea9a507b0",
   "metadata": {},
   "source": [
    "##### Install the requirements for LocalGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deadcf84-eb2b-4f04-9cbf-4e6d67d4e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,512 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,036 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,282 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1,792 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,572 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,304 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,576 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [78.3 kB]\n",
      "Fetched 28.8 MB in 4s (7,687 kB/s)                           \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "bc is already the newest version (1.07.1-3build1).\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
      "openssh-server is already the newest version (1:8.9p1-3ubuntu0.4).\n",
      "tzdata is already the newest version (2023c-0ubuntu0.22.04.2).\n",
      "tzdata set to manually installed.\n",
      "vim is already the newest version (2:8.2.3995-1ubuntu2.13).\n",
      "protobuf-compiler is already the newest version (3.12.4-1ubuntu7.22.04.1).\n",
      "python3-pip is already the newest version (22.0.2+dfsg-1ubuntu0.4).\n",
      "The following additional packages will be installed:\n",
      "  libcurl4 libcurl4-openssl-dev libsigsegv2 libutempter0\n",
      "Suggested packages:\n",
      "  gawk-doc libcurl4-doc libidn11-dev libkrb5-dev libldap2-dev librtmp-dev\n",
      "  libssh2-1-dev\n",
      "The following NEW packages will be installed:\n",
      "  bash-completion gawk iputils-ping libsigsegv2 libutempter0 net-tools tmux\n",
      "The following packages will be upgraded:\n",
      "  curl libcurl4 libcurl4-openssl-dev\n",
      "3 upgraded, 7 newly installed, 0 to remove and 33 not upgraded.\n",
      "Need to get 2,195 kB of archives.\n",
      "After this operation, 5,307 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsigsegv2 amd64 2.13-1ubuntu3 [14.6 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gawk amd64 1:5.1.0-1ubuntu0.1 [447 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 iputils-ping amd64 3:20211215-1 [42.9 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 bash-completion all 1:2.11-5ubuntu1 [180 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcurl4-openssl-dev amd64 7.81.0-1ubuntu1.15 [386 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 curl amd64 7.81.0-1ubuntu1.15 [194 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcurl4 amd64 7.81.0-1ubuntu1.15 [289 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libutempter0 amd64 1.2.1-2build2 [8,848 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 net-tools amd64 1.60+git20181103.0eebece-1ubuntu5 [204 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 tmux amd64 3.2a-4ubuntu0.2 [428 kB]\n",
      "Fetched 2,195 kB in 1s (2,374 kB/s)\n",
      "Selecting previously unselected package libsigsegv2:amd64.\n",
      "(Reading database ... 49958 files and directories currently installed.)\n",
      "Preparing to unpack .../libsigsegv2_2.13-1ubuntu3_amd64.deb ...\n",
      "Unpacking libsigsegv2:amd64 (2.13-1ubuntu3) ...\n",
      "Setting up libsigsegv2:amd64 (2.13-1ubuntu3) ...\n",
      "Selecting previously unselected package gawk.\n",
      "(Reading database ... 49965 files and directories currently installed.)\n",
      "Preparing to unpack .../0-gawk_1%3a5.1.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking gawk (1:5.1.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package iputils-ping.\n",
      "Preparing to unpack .../1-iputils-ping_3%3a20211215-1_amd64.deb ...\n",
      "Unpacking iputils-ping (3:20211215-1) ...\n",
      "Selecting previously unselected package bash-completion.\n",
      "Preparing to unpack .../2-bash-completion_1%3a2.11-5ubuntu1_all.deb ...\n",
      "Unpacking bash-completion (1:2.11-5ubuntu1) ...\n",
      "Preparing to unpack .../3-libcurl4-openssl-dev_7.81.0-1ubuntu1.15_amd64.deb ...\n",
      "Unpacking libcurl4-openssl-dev:amd64 (7.81.0-1ubuntu1.15) over (7.81.0-1ubuntu1.14) ...\n",
      "Preparing to unpack .../4-curl_7.81.0-1ubuntu1.15_amd64.deb ...\n",
      "Unpacking curl (7.81.0-1ubuntu1.15) over (7.81.0-1ubuntu1.14) ...\n",
      "Preparing to unpack .../5-libcurl4_7.81.0-1ubuntu1.15_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.81.0-1ubuntu1.15) over (7.81.0-1ubuntu1.14) ...\n",
      "Selecting previously unselected package libutempter0:amd64.\n",
      "Preparing to unpack .../6-libutempter0_1.2.1-2build2_amd64.deb ...\n",
      "Unpacking libutempter0:amd64 (1.2.1-2build2) ...\n",
      "Selecting previously unselected package net-tools.\n",
      "Preparing to unpack .../7-net-tools_1.60+git20181103.0eebece-1ubuntu5_amd64.deb ...\n",
      "Unpacking net-tools (1.60+git20181103.0eebece-1ubuntu5) ...\n",
      "Selecting previously unselected package tmux.\n",
      "Preparing to unpack .../8-tmux_3.2a-4ubuntu0.2_amd64.deb ...\n",
      "Unpacking tmux (3.2a-4ubuntu0.2) ...\n",
      "Setting up net-tools (1.60+git20181103.0eebece-1ubuntu5) ...\n",
      "Setting up gawk (1:5.1.0-1ubuntu0.1) ...\n",
      "Setting up bash-completion (1:2.11-5ubuntu1) ...\n",
      "Setting up libutempter0:amd64 (1.2.1-2build2) ...\n",
      "Setting up libcurl4:amd64 (7.81.0-1ubuntu1.15) ...\n",
      "Setting up curl (7.81.0-1ubuntu1.15) ...\n",
      "Setting up iputils-ping (3:20211215-1) ...\n",
      "Setting up tmux (3.2a-4ubuntu0.2) ...\n",
      "Setting up libcurl4-openssl-dev:amd64 (7.81.0-1ubuntu1.15) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y tzdata bash-completion python3-pip openssh-server      vim git iputils-ping net-tools protobuf-compiler curl bc gawk tmux     && rm -rf /var/lib/apt/lists/*\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d111e-2118-4bb0-b62d-214e895479a2",
   "metadata": {},
   "source": [
    "##### Install the Optimum Habana Library from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217c2833-0bb7-481e-907d-7b28d88bbf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade-strategy eager optimum[habana]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2eaba6-2d4b-4007-bb13-ca5113022aad",
   "metadata": {},
   "source": [
    "### Load your Local Content\n",
    "Copy all of your files into the `SOURCE_DOCUMENTS` directory\n",
    "\n",
    "The current default file types are .txt, .pdf, .csv, and .xlsx, if you want to use any other file type, you will need to convert it to one of the default file types.\n",
    "\n",
    "Run the following command to ingest all the data. The ingest.py uses LangChain tools to parse the document and create embeddings locally using the GaudiHuggingFaceEmbeddings class. It then stores the result in a local vector database (DB) using Chroma vector store. \n",
    "\n",
    "If you want to start from an empty database, delete the DB folder and run the ingest script again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db570f4-acce-4e73-8ac1-7742e621c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-09 00:42:28,492 - INFO - ingest.py:124 - Loading documents from /root/Gaudi-tutorials/PyTorch/localGPT_inference/SOURCE_DOCUMENTS\n",
      "2023-12-09 00:42:28,508 - INFO - ingest.py:37 - Loading document batch\n",
      "2023-12-09 00:42:30,371 - INFO - ingest.py:133 - Loaded 1 documents from /root/Gaudi-tutorials/PyTorch/localGPT_inference/SOURCE_DOCUMENTS\n",
      "2023-12-09 00:42:30,371 - INFO - ingest.py:134 - Split into 72 chunks of text\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "Loading Habana modules from /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/lib\n",
      "2023-12-09 00:42:32,089 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Downloading .gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 3.22MB/s]\n",
      "Downloading 1_Pooling/config.json: 100%|████████| 190/190 [00:00<00:00, 630kB/s]\n",
      "Downloading README.md: 100%|███████████████| 10.6k/10.6k [00:00<00:00, 26.1MB/s]\n",
      "Downloading config.json: 100%|█████████████████| 612/612 [00:00<00:00, 2.03MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████| 116/116 [00:00<00:00, 394kB/s]\n",
      "Downloading data_config.json: 100%|████████| 39.3k/39.3k [00:00<00:00, 17.3MB/s]\n",
      "Downloading pytorch_model.bin: 100%|████████| 90.9M/90.9M [00:00<00:00, 164MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|████| 53.0/53.0 [00:00<00:00, 410kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 112/112 [00:00<00:00, 848kB/s]\n",
      "Downloading tokenizer.json: 100%|████████████| 466k/466k [00:00<00:00, 2.52MB/s]\n",
      "Downloading tokenizer_config.json: 100%|███████| 350/350 [00:00<00:00, 1.16MB/s]\n",
      "Downloading train_script.py: 100%|█████████| 13.2k/13.2k [00:00<00:00, 62.4MB/s]\n",
      "Downloading vocab.txt: 100%|█████████████████| 232k/232k [00:00<00:00, 1.89MB/s]\n",
      "Downloading modules.json: 100%|████████████████| 349/349 [00:00<00:00, 2.67MB/s]\n",
      "2023-12-09 00:42:36,740 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446912 KB\n",
      "------------------------------------------------------------------------------\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:01<00:00,  2.17it/s]\n",
      "2023-12-09 00:42:43,037 - INFO - ingest.py:161 - Time taken to create embeddings vectorstore: 6.009458071028348s\n"
     ]
    }
   ],
   "source": [
    "!python ingest.py --device_type hpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03495bec-61c5-4b49-a2ee-387c75ba088e",
   "metadata": {},
   "source": [
    "### How to access and Use the Llama 2 model\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses.\n",
    "\n",
    "To be able to run gated models like this Llama-2-70b-chat-hf, you need the following:\n",
    "\n",
    "* Have a HuggingFace account\n",
    "* Agree to the terms of use of the model in its model card on the HF Hub\n",
    "* Set a read token\n",
    "* Login to your account using the HF CLI: run huggingface-cli login before launching your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb59c62-41c4-45b5-a642-3b61b341d135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#!huggingface-cli login --token <your token here>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3ac741b-cb79-4fab-a1aa-efa075503d61",
   "metadata": {},
   "source": [
    "### Running the LocalGPT model with Llama2 70B Chat \n",
    "\n",
    "### Set the model Usage\n",
    "\n",
    "To change the model, you can modify the \"LLM_ID = <add model here>\" in the `constants.py` file. For this example, the default is `meta-llama/Llama-2-70b-chat-hf`.  \n",
    "\n",
    "Since this is interactive, it's a better experince to launch this from a terminal window.  This run_localGPT.py script uses a local LLM (Llama 2 in this case) to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the documentation.  This is the run command to use:\n",
    "\n",
    "`PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`\n",
    "\n",
    "Running the full 70B model takes up ~128GB of disk space, so if your system is storage constrained, it may be best to run the Llama 2 7B or 13B chat models.  Change the LLM_ID variable in the `constants.py` file (example: `LLM_ID = \"meta-llama/Llama-2-7b-chat-hf\"`) and use the command below.\n",
    "`python run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`\n",
    "\n",
    "Note: The inference is running sampling mode, so the user can optinally modify the temperature and top_p settings.  The current settings are temperature=0.7, top_p=0.95.  Type \"exit\" at the prompt to stop the execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee22fd-2bca-49b8-8f8d-501075058b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this command in a terminal window to start the interactive chat: `PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`, the example below is showing the initial output:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc237333-fbdc-440e-8249-89639af4c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank run_localGPT.py --device_type hpu --temperature 0.2 --top_p 0.95\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2023-12-09 01:30:35,163] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:36,512] [WARNING] [runner.py:206:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-12-09 01:30:36,575] [INFO] [runner.py:585:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None run_localGPT.py --device_type hpu --temperature 0.2 --top_p 0.95\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2023-12-09 01:30:38,987] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:164:main] dist_world_size=8\n",
      "[2023-12-09 01:30:40,339] [INFO] [launch.py:166:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:43,974 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "2023-12-09 01:30:43,978 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:43,978 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:43,979 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:43,979 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:43,979 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,065 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,118 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,119 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,119 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,119 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,119 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,197 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,302 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "2023-12-09 01:30:44,330 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,330 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,331 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,331 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,331 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:202 - Running on: hpu\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:203 - Display Source Documents set to: False\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:38 - temperature set to 0.2, top_p set to 0.95\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:39 - Loading Model: meta-llama/Llama-2-70b-chat-hf, on: hpu\n",
      "2023-12-09 01:30:44,566 - INFO - run_localGPT.py:40 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2023-12-09 01:30:47,114] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,195] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,347] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,356] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,446] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,585] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,864] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:47,874] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-12-09 01:30:48,389] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,389] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:48,492] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,492] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:48,492] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl\n",
      "[2023-12-09 01:30:48,498] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,498] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:48,588] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,588] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:48,696] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:48,696] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:49,026] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:49,026] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:49,198] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:49,198] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-09 01:30:49,569] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2023-12-09 01:30:49,569] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 76912.67it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 70138.86it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 39028.88it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 29916.58it/s]\n",
      "Fetching 15 files: 100%|██████████████████████| 15/15 [00:00<00:00, 7181.21it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 60436.66it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 22098.55it/s]\n",
      "Fetching 15 files: 100%|█████████████████████| 15/15 [00:00<00:00, 66156.21it/s]\n",
      "Fetching 15 files: 100%|████████████████████| 15/15 [00:00<00:00, 125078.65it/s]\n",
      "[2023-12-09 01:31:14,039] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3+hpu.synapse.v1.13.0, git-hash=6522014, git-branch=1.13.0\n",
      "[2023-12-09 01:31:14,042] [INFO] [logging.py:96:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Loading 15 checkpoint shards:   0%|                      | 0/15 [00:00<?, ?it/s]============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446912 KB\n",
      "------------------------------------------------------------------------------\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:10<00:00,  4.67s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:13<00:00,  4.88s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:19<00:00,  5.29s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:21<00:00,  5.43s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:23<00:00,  5.60s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:25<00:00,  5.69s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:26<00:00,  5.78s/it]\n",
      "Loading 15 checkpoint shards: 100%|█████████████| 15/15 [01:29<00:00,  5.95s/it]\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,013 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,014 - INFO - run_localGPT.py:137 - Local LLM Loaded\n",
      "2023-12-09 01:32:58,176 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,178 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,242 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,242 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,242 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,242 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,247 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,255 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:58,582 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:32:59,854 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,164 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,541 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,553 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,582 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,638 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-09 01:33:00,639 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "\n",
      "Enter a query: "
     ]
    }
   ],
   "source": [
    "!PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.2 --top_p 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe1723-04da-4d59-a299-fe0431a84bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba45e02-5a1f-4dd0-863e-5bdf8e2cbca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
