{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646a18d1-97cd-4ffc-8394-3bff4e8bef6e",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dbf1e-14f5-4b1a-a67a-0859d88bf942",
   "metadata": {},
   "source": [
    "# Using LocalGPT on the Intel&reg; Gaudi&reg; 2 AI accelerator with the Llama2 70B model\n",
    "This tutorial will show how to use the [LocalGPT](https://github.com/PromtEngineer/localGPT) open source initiative on the Intel Gaudi 2 AI accelerator.  LocalGPT allows you to load your own documents and run an interactive chat session with this material.  This allows you to query and summarize your content by loading any .pdf or .txt documents into the `SOURCE DOCUMENTS` folder, running the ingest.py script to tokenize your content and then the run_localGPT.py script to start the interaction.  \n",
    "\n",
    "In this example, we're using the **meta-llama/Llama-2-70b-chat-hf** model as the refrence model that will manage the inference on Gaudi 2.  DeepSpeed inference is used based on the size of the model.\n",
    "\n",
    "To optimize this instantiation of LocalGPT, we have created new content on top of the existing Hugging Face based \"text-generation\" inference task and pipelines, including:\n",
    "\n",
    "1. Using the Hugging Face Optimum Habana Library with the Llama2-70B model, which is optimized on Gaudi2. \n",
    "2. Using Langchain to import the source docuement with a custom embedding model, using a `GaudiHuggingFaceEmbeddings` class based on HuggingFaceEmbeddings.\n",
    "3. We are using a custom pipeline class, `GaudiTextGenerationPipeline` that optimizes text-generation tasks for padding and indexing for static shapes, to improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be507912-770e-4920-b939-a495d29e2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Install DeepSpeed to run inference on the full Llama2 70B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7fd64-18b0-4a71-8484-813defa94062",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35741c-da8a-49e3-9bea-182d19233f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Go to the LocalGPT folder and set environment varialbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c604050-d6f5-47a9-adc7-0b9e3c0fe34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/localGPT_inference\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi-tutorials/PyTorch/localGPT_inference\n",
    "!export DEBIAN_FRONTEND=\"noninteractive\"\n",
    "!export TZ=Etc/UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e2383-deaf-4349-90e5-31bea9a507b0",
   "metadata": {},
   "source": [
    "##### Install the requirements for LocalGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deadcf84-eb2b-4f04-9cbf-4e6d67d4e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1,792 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,036 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,538 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,304 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,552 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [78.3 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,512 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,282 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
      "Fetched 28.7 MB in 5s (6,026 kB/s)   \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "bash-completion is already the newest version (1:2.11-5ubuntu1).\n",
      "bc is already the newest version (1.07.1-3build1).\n",
      "iputils-ping is already the newest version (3:20211215-1).\n",
      "net-tools is already the newest version (1.60+git20181103.0eebece-1ubuntu5).\n",
      "gawk is already the newest version (1:5.1.0-1ubuntu0.1).\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
      "openssh-server is already the newest version (1:8.9p1-3ubuntu0.4).\n",
      "tmux is already the newest version (3.2a-4ubuntu0.2).\n",
      "tzdata is already the newest version (2023c-0ubuntu0.22.04.2).\n",
      "vim is already the newest version (2:8.2.3995-1ubuntu2.13).\n",
      "protobuf-compiler is already the newest version (3.12.4-1ubuntu7.22.04.1).\n",
      "python3-pip is already the newest version (22.0.2+dfsg-1ubuntu0.4).\n",
      "The following additional packages will be installed:\n",
      "  libcurl4 libcurl4-openssl-dev\n",
      "Suggested packages:\n",
      "  libcurl4-doc libidn11-dev libkrb5-dev libldap2-dev librtmp-dev libssh2-1-dev\n",
      "The following packages will be upgraded:\n",
      "  curl libcurl4 libcurl4-openssl-dev\n",
      "3 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
      "Need to get 869 kB of archives.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcurl4-openssl-dev amd64 7.81.0-1ubuntu1.15 [386 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 curl amd64 7.81.0-1ubuntu1.15 [194 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libcurl4 amd64 7.81.0-1ubuntu1.15 [289 kB]\n",
      "Fetched 869 kB in 1s (583 kB/s)   \n",
      "(Reading database ... 50959 files and directories currently installed.)\n",
      "Preparing to unpack .../libcurl4-openssl-dev_7.81.0-1ubuntu1.15_amd64.deb ...\n",
      "Unpacking libcurl4-openssl-dev:amd64 (7.81.0-1ubuntu1.15) over (7.81.0-1ubuntu1.14) ...\n",
      "Preparing to unpack .../curl_7.81.0-1ubuntu1.15_amd64.deb ...\n",
      "Unpacking curl (7.81.0-1ubuntu1.15) over (7.81.0-1ubuntu1.14) ...\n",
      "Preparing to unpack .../libcurl4_7.81.0-1ubuntu1.15_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.81.0-1ubuntu1.15) over (7.81.0-1ubuntu1.14) ...\n",
      "Setting up libcurl4:amd64 (7.81.0-1ubuntu1.15) ...\n",
      "Setting up curl (7.81.0-1ubuntu1.15) ...\n",
      "Setting up libcurl4-openssl-dev:amd64 (7.81.0-1ubuntu1.15) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y tzdata bash-completion python3-pip openssh-server      vim git iputils-ping net-tools protobuf-compiler curl bc gawk tmux     && rm -rf /var/lib/apt/lists/*\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d111e-2118-4bb0-b62d-214e895479a2",
   "metadata": {},
   "source": [
    "##### Install the Optimum Habana Library from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217c2833-0bb7-481e-907d-7b28d88bbf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade-strategy eager optimum[habana]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2eaba6-2d4b-4007-bb13-ca5113022aad",
   "metadata": {},
   "source": [
    "### Load your Local Content\n",
    "Copy all of your files into the `SOURCE_DOCUMENTS` directory\n",
    "\n",
    "The current default file types are .txt, .pdf, .csv, and .xlsx, if you want to use any other file type, you will need to convert it to one of the default file types.\n",
    "\n",
    "Run the following command to ingest all the data. The ingest.py uses LangChain tools to parse the document and create embeddings locally using the GaudiHuggingFaceEmbeddings class. It then stores the result in a local vector database (DB) using Chroma vector store. \n",
    "\n",
    "If you want to start from an empty database, delete the DB folder and run the ingest script again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db570f4-acce-4e73-8ac1-7742e621c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-08 21:22:12,496 - INFO - ingest.py:124 - Loading documents from /root/Gaudi-tutorials/PyTorch/localGPT_inference/SOURCE_DOCUMENTS\n",
      "2023-12-08 21:22:12,509 - INFO - ingest.py:37 - Loading document batch\n",
      "2023-12-08 21:22:14,789 - INFO - ingest.py:133 - Loaded 1 documents from /root/Gaudi-tutorials/PyTorch/localGPT_inference/SOURCE_DOCUMENTS\n",
      "2023-12-08 21:22:14,790 - INFO - ingest.py:134 - Split into 72 chunks of text\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "Loading Habana modules from /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/lib\n",
      "2023-12-08 21:22:15,898 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-08 21:22:16,304 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "fopen of /sys/class/accel/accel6 (deleted)/device/pci_addr failed\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446912 KB\n",
      "------------------------------------------------------------------------------\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:01<00:00,  2.15it/s]\n",
      "2023-12-08 21:22:21,804 - INFO - ingest.py:161 - Time taken to create embeddings vectorstore: 5.306310297019081s\n"
     ]
    }
   ],
   "source": [
    "!python ingest.py --device_type hpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03495bec-61c5-4b49-a2ee-387c75ba088e",
   "metadata": {},
   "source": [
    "### How to access and Use the Llama 2 model\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses.\n",
    "\n",
    "To be able to run gated models like this Llama-2-70b-chat-hf, you need the following:\n",
    "\n",
    "* Have a HuggingFace account\n",
    "* Agree to the terms of use of the model in its model card on the HF Hub\n",
    "* Set a read token\n",
    "* Login to your account using the HF CLI: run huggingface-cli login before launching your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb59c62-41c4-45b5-a642-3b61b341d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token <your token here>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3ac741b-cb79-4fab-a1aa-efa075503d61",
   "metadata": {},
   "source": [
    "### Running the LocalGPT model with Llama2 70B Chat \n",
    "\n",
    "### Set the model Usage\n",
    "\n",
    "To change the model, you can modify the \"LLM_ID = <add model here>\" in the `constants.py` file. For this example, the default is `meta-llama/Llama-2-70b-chat-hf`.  \n",
    "\n",
    "Since this is interactive, it's a better experince to launch this from a terminal window.  This run_localGPT.py script uses a local LLM (Llama 2 in this case) to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the documentation.  This is the run command to use:\n",
    "\n",
    "`PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`\n",
    "\n",
    "To run the Llama 2 7B or 13B chat models, you can change the LLM_ID variable in the `constants.py` file (example: `LLM_ID = \"meta-llama/Llama-2-70b-chat-hf\"`) and use the command below.\n",
    "`python run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`\n",
    "\n",
    "Note: The inference is running sampling mode, so the user can optinally modify the temperature and top_p settings.  The current settings are temperature=0.7, top_p=0.95.  Type \"exit\" at the prompt to stop the execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2abdc3-850e-4f55-8dc4-6f90d47bfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a terminal window to start the chat: `PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`, the example below is showing the initial output:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc237333-fbdc-440e-8249-89639af4c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python gaudi_spawn.py --use_deepspeed --world_size 8 run_localGPT.py --device_type hpu --temperature 0.2 --top_p 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe1723-04da-4d59-a299-fe0431a84bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba45e02-5a1f-4dd0-863e-5bdf8e2cbca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
