{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0dc54a-50cc-412b-b0c7-42b3ccf8f1dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca3972",
   "metadata": {},
   "source": [
    "# Distributed Training Using TensorFlow and HPUStrategy\n",
    "This tutorial demonstrates how distributed training works with [HPUStrategy](https://docs.habana.ai/en/latest/Tensorflow_Scaling_Guide/TensorFlow_Gaudi_Scaling_Guide.html#multi-worker-training-using-hpustrategy) using Habana Gaudi AI processors.\n",
    "\n",
    "[tf.distribute.Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) is a TensorFlow API to distribute training across multiple Gaudi devices, and multiple machines. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
    "\n",
    "To demonstrate distributed training, we will train a simple Keras model on the [MNIST database](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "You can find more information on distributed training using TensorFlow and HPUStrategy on [Multi-Worker Training using HPUStrategy Guide](https://docs.habana.ai/en/latest/Tensorflow_Scaling_Guide/TensorFlow_Gaudi_Scaling_Guide.html#multi-worker-training-using-hpustrategy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a0834",
   "metadata": {},
   "source": [
    "## Start MPI engines in Jupiter notebook\n",
    "MPI is used for coordinating work between processes. You can find a simple example of how to initialize MPI and run the model using the command \"mpirun\" [here](https://github.com/HabanaAI/Model-References/tree/master/TensorFlow/examples/distribute_with_hpu_strategy).\n",
    "\n",
    "You can find more information on the [Open MPI website](https://www.open-mpi.org/).\n",
    "\n",
    "[ipyparallel](https://ipyparallel.readthedocs.io/en/latest/) and [mpi4py](https://mpi4py.readthedocs.io/en/stable/) are required to use MPI from the Jupiter notebook, If they have not been installed, install them using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a101b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment next lines if needed\n",
    "# !pip install jupyter\n",
    "# !pip install ipyparallel\n",
    "# !pip install mpi4py\n",
    "# !pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1489dc-fa3f-49d9-bc31-712d6cc4fb14",
   "metadata": {},
   "source": [
    "First, import the ipyparallel package, and then start the MPI engines.\n",
    "\n",
    "In our example, we will start 8 MPI engines to use all the 8 Gaudi devices in our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436d784e-0dd8-47fd-be12-6b43f3284b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 8 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075eeab1bbd54fdd9ed7baac0743de52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "import os\n",
    "os.environ[\"OMPI_ALLOW_RUN_AS_ROOT\"] = \"1\"\n",
    "os.environ[\"OMPI_ALLOW_RUN_AS_ROOT_CONFIRM\"] = \"1\"\n",
    "\n",
    "n_hpu=8\n",
    "cluster = ipp.Cluster(engines='mpi', n=n_hpu)\n",
    "client = cluster.start_and_connect_sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad83db8-e990-4eca-be46-5328d203f199",
   "metadata": {},
   "source": [
    "## Execute Python commands in parallel\n",
    "The [%%px cell magic](https://ipython.org/ipython-doc/3/parallel/magics.html#px-cell-magic) is used to execute Python command on all the MPI engines in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9ced2-92d7-466d-add8-4c31cb17af95",
   "metadata": {},
   "source": [
    "## Import TensorFlow\n",
    "The MPI engines have been started. The following scripts will import the TensorFlow library in each engine in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858328d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68121d7f1c634a57996b4a68f957806c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/8 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a454b-14db-40da-b9b1-69ab46bcfaaa",
   "metadata": {},
   "source": [
    "## Import and enable Habana TensorFlow module\n",
    "Letâ€™s enable Gaudi devices by loading the Habana module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7b0cc7-46df-430f-b190-a8b5bd0204c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stderr:1] WARNING:/usr/local/lib/python3.8/dist-packages/habana_frameworks/tensorflow/library_loader.py:Habana-TensorFlow(1.2.0) and Habanalabs Driver(1.3.0-e793625) versions differ!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:7] WARNING:/usr/local/lib/python3.8/dist-packages/habana_frameworks/tensorflow/library_loader.py:Habana-TensorFlow(1.2.0) and Habanalabs Driver(1.3.0-e793625) versions differ!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:5] WARNING:/usr/local/lib/python3.8/dist-packages/habana_frameworks/tensorflow/library_loader.py:Habana-TensorFlow(1.2.0) and Habanalabs Driver(1.3.0-e793625) versions differ!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ae9f74ecee4324a969597744144b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/8 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:2] WARNING:/usr/local/lib/python3.8/dist-packages/habana_frameworks/tensorflow/library_loader.py:Habana-TensorFlow(1.2.0) and Habanalabs Driver(1.3.0-e793625) versions differ!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:4] WARNING:/usr/local/lib/python3.8/dist-packages/habana_frameworks/tensorflow/library_loader.py:Habana-TensorFlow(1.2.0) and Habanalabs Driver(1.3.0-e793625) versions differ!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:3] WARNING:/usr/local/lib/python3.8/dist-packages/habana_frameworks/tensorflow/library_loader.py:Habana-TensorFlow(1.2.0) and Habanalabs Driver(1.3.0-e793625) versions differ!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] WARNING:/usr/local/lib/python3.8/dist-packages/habana_frameworks/tensorflow/library_loader.py:Habana-TensorFlow(1.2.0) and Habanalabs Driver(1.3.0-e793625) versions differ!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:6] WARNING:/usr/local/lib/python3.8/dist-packages/habana_frameworks/tensorflow/library_loader.py:Habana-TensorFlow(1.2.0) and Habanalabs Driver(1.3.0-e793625) versions differ!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "from habana_frameworks.tensorflow import load_habana_module\n",
    "load_habana_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f20c1a-f576-4bee-a6ef-51ddd3f01ba4",
   "metadata": {},
   "source": [
    "## Set TF_CONFIG\n",
    "TensorFlow uses the TF_CONFIG environment variable to facilitate distributed training. Define a helper function to set up the TF_CONFIG environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a8acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "BASE_TF_SERVER_PORT = 7850\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "\n",
    "num_workers = MPI.COMM_WORLD.Get_size()\n",
    "worker_index = MPI.COMM_WORLD.Get_rank()\n",
    "\n",
    "def set_tf_config():\n",
    "    \"\"\" Makes a TensorFlow cluster information and sets it to TF_CONFIG environment variable.\n",
    "    \"\"\"\n",
    "    tf_config = {\n",
    "        \"cluster\": {\n",
    "            \"worker\": [f\"localhost:{BASE_TF_SERVER_PORT + index}\" for index in range(num_workers)]\n",
    "        },\n",
    "        \"task\": {\"type\": \"worker\", \"index\": worker_index}\n",
    "    }\n",
    "    tf_config_text = json.dumps(tf_config)\n",
    "    os.environ[\"TF_CONFIG\"] = tf_config_text\n",
    "    print(f\"TF_CONFIG = {tf_config_text}\")\n",
    "    return tf_config_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dfd9b9",
   "metadata": {},
   "source": [
    "## Create a training function\n",
    "To train the model on multiple Gaudi devices, import the HPUStrategy from habana_frameworks.tensorflow.distribute, and set the strategy to be HPUStrategy.\n",
    "Remember to create a model and compile it with the strategy.scope() for distributed training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b25d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def train_mnist(batch_size: int, num_epochs: int):\n",
    "    \"\"\" Train the distributed model on MNIST Dataset.\n",
    "    \"\"\"\n",
    "    # Set TF_CONFIG.\n",
    "    set_tf_config()\n",
    "    # Instantiate the distributed strategy class.\n",
    "    # Use HPUStrategy \n",
    "    from habana_frameworks.tensorflow.distribute import HPUStrategy\n",
    "    strategy = HPUStrategy()\n",
    "    # Determine the total training batch size.\n",
    "    batch_size_per_replica = batch_size\n",
    "    total_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "    print(\n",
    "        f\"total_batch_size = {batch_size_per_replica} * {strategy.num_replicas_in_sync} workers = {total_batch_size}\")\n",
    "    # Load and preprocess the MNIST Dataset.\n",
    "    # As tfds.load() may download the dataset if not cached, let the first worker do it first.\n",
    "    for dataload_turn in range(2):\n",
    "        if (dataload_turn == 0) == (worker_index == 0):\n",
    "            print(\"Loading MNIST dataset...\")\n",
    "            datasets, info = tfds.load(\n",
    "                name=\"mnist\", with_info=True, as_supervised=True)\n",
    "        MPI.COMM_WORLD.barrier()\n",
    "    def preprocess(image, label):\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        label = tf.cast(label, tf.int32)\n",
    "        return image, label\n",
    "    train_dataset = datasets[\"train\"]\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    train_dataset = train_dataset.with_options(options)\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess).cache().shuffle(SHUFFLE_BUFFER_SIZE).batch(total_batch_size)\n",
    "    test_dataset = datasets[\"test\"]\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    test_dataset = test_dataset.with_options(options)\n",
    "    test_dataset = test_dataset.map(\n",
    "        preprocess).batch(total_batch_size)\n",
    "    # Create and compile the distributed CNN model.\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(\n",
    "                32, 3, activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ])\n",
    "        model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      optimizer=tf.keras.optimizers.Adam(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "        \n",
    "    # Train the model.\n",
    "    print(\"Calling model.fit()...\")\n",
    "    model.fit(train_dataset, epochs=num_epochs, verbose=2)\n",
    "    print(\"Calling model.evaluate()...\")\n",
    "    eval_results = model.evaluate(test_dataset, verbose=2)\n",
    "    print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd14be",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "It's time to call the training function built above to run model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce022ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:7850\", \"localhost:7851\", \"localhost:7852\", \"localhost:7853\", \"localhost:7854\", \"localhost:7855\", \"localhost:7856\", \"localhost:7857\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}\n",
       "   local_devices=('/job:worker/task:0/device:HPU:0',)\n",
       "total_batch_size = 64 * 8 workers = 512\n",
       "Loading MNIST dataset...\n",
       "Calling model.fit()...\n",
       "Epoch 1/2\n",
       "118/118 - 7s - loss: 0.4593 - accuracy: 0.8770 - 7s/epoch - 62ms/step\n",
       "Epoch 2/2\n",
       "118/118 - 1s - loss: 0.1515 - accuracy: 0.9567 - 922ms/epoch - 8ms/step\n",
       "Calling model.evaluate()...\n",
       "20/20 - 2s - loss: 0.1042 - accuracy: 0.9664 - 2s/epoch - 84ms/step\n",
       "Evaluation results: [0.10418514907360077, 0.9664062261581421]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:6] TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:7850\", \"localhost:7851\", \"localhost:7852\", \"localhost:7853\", \"localhost:7854\", \"localhost:7855\", \"localhost:7856\", \"localhost:7857\"]}, \"task\": {\"type\": \"worker\", \"index\": 6}}\n",
       "   local_devices=('/job:worker/task:6/device:HPU:0',)\n",
       "total_batch_size = 64 * 8 workers = 512\n",
       "Loading MNIST dataset...\n",
       "Calling model.fit()...\n",
       "Epoch 1/2\n",
       "118/118 - 7s - loss: 0.4593 - accuracy: 0.8770 - 7s/epoch - 62ms/step\n",
       "Epoch 2/2\n",
       "118/118 - 1s - loss: 0.1515 - accuracy: 0.9567 - 922ms/epoch - 8ms/step\n",
       "Calling model.evaluate()...\n",
       "20/20 - 2s - loss: 0.1042 - accuracy: 0.9664 - 2s/epoch - 85ms/step\n",
       "Evaluation results: [0.10418514907360077, 0.9664062261581421]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:7] TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:7850\", \"localhost:7851\", \"localhost:7852\", \"localhost:7853\", \"localhost:7854\", \"localhost:7855\", \"localhost:7856\", \"localhost:7857\"]}, \"task\": {\"type\": \"worker\", \"index\": 7}}\n",
       "   local_devices=('/job:worker/task:7/device:HPU:0',)\n",
       "total_batch_size = 64 * 8 workers = 512\n",
       "Loading MNIST dataset...\n",
       "Calling model.fit()...\n",
       "Epoch 1/2\n",
       "118/118 - 7s - loss: 0.4593 - accuracy: 0.8770 - 7s/epoch - 62ms/step\n",
       "Epoch 2/2\n",
       "118/118 - 1s - loss: 0.1515 - accuracy: 0.9567 - 922ms/epoch - 8ms/step\n",
       "Calling model.evaluate()...\n",
       "20/20 - 2s - loss: 0.1042 - accuracy: 0.9664 - 2s/epoch - 85ms/step\n",
       "Evaluation results: [0.10418514907360077, 0.9664062261581421]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] 2022-01-22 08:08:42.777161: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2022-01-22 08:08:44.329198: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.334855: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.339890: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:7850, 1 -> localhost:7851, 2 -> localhost:7852, 3 -> localhost:7853, 4 -> localhost:7854, 5 -> localhost:7855, 6 -> localhost:7856, 7 -> localhost:7857}\n",
       "2022-01-22 08:08:44.342238: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:7850\n",
       "2022-01-22 08:08:44.740452: I /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/synapse_helpers/hccl_communicator.cpp:55] Opening communication. Device id:0.\n",
       "2022-01-22 08:08:57.200230: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:6] 2022-01-22 08:08:42.777690: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2022-01-22 08:08:44.360132: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.362736: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.367844: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:7850, 1 -> localhost:7851, 2 -> localhost:7852, 3 -> localhost:7853, 4 -> localhost:7854, 5 -> localhost:7855, 6 -> localhost:7856, 7 -> localhost:7857}\n",
       "2022-01-22 08:08:44.369731: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:7856\n",
       "2022-01-22 08:08:44.738346: I /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/synapse_helpers/hccl_communicator.cpp:55] Opening communication. Device id:0.\n",
       "2022-01-22 08:08:57.200009: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:7850\", \"localhost:7851\", \"localhost:7852\", \"localhost:7853\", \"localhost:7854\", \"localhost:7855\", \"localhost:7856\", \"localhost:7857\"]}, \"task\": {\"type\": \"worker\", \"index\": 2}}\n",
       "   local_devices=('/job:worker/task:2/device:HPU:0',)\n",
       "total_batch_size = 64 * 8 workers = 512\n",
       "Loading MNIST dataset...\n",
       "Calling model.fit()...\n",
       "Epoch 1/2\n",
       "118/118 - 7s - loss: 0.4593 - accuracy: 0.8770 - 7s/epoch - 62ms/step\n",
       "Epoch 2/2\n",
       "118/118 - 1s - loss: 0.1515 - accuracy: 0.9567 - 922ms/epoch - 8ms/step\n",
       "Calling model.evaluate()...\n",
       "20/20 - 2s - loss: 0.1042 - accuracy: 0.9664 - 2s/epoch - 84ms/step\n",
       "Evaluation results: [0.10418514907360077, 0.9664062261581421]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:7] 2022-01-22 08:08:42.778841: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2022-01-22 08:08:44.363850: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.368635: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.373796: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:7850, 1 -> localhost:7851, 2 -> localhost:7852, 3 -> localhost:7853, 4 -> localhost:7854, 5 -> localhost:7855, 6 -> localhost:7856, 7 -> localhost:7857}\n",
       "2022-01-22 08:08:44.375903: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:7857\n",
       "2022-01-22 08:08:44.740024: I /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/synapse_helpers/hccl_communicator.cpp:55] Opening communication. Device id:0.\n",
       "2022-01-22 08:08:57.199857: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:4] TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:7850\", \"localhost:7851\", \"localhost:7852\", \"localhost:7853\", \"localhost:7854\", \"localhost:7855\", \"localhost:7856\", \"localhost:7857\"]}, \"task\": {\"type\": \"worker\", \"index\": 4}}\n",
       "   local_devices=('/job:worker/task:4/device:HPU:0',)\n",
       "total_batch_size = 64 * 8 workers = 512\n",
       "Loading MNIST dataset...\n",
       "Calling model.fit()...\n",
       "Epoch 1/2\n",
       "118/118 - 7s - loss: 0.4593 - accuracy: 0.8770 - 7s/epoch - 62ms/step\n",
       "Epoch 2/2\n",
       "118/118 - 1s - loss: 0.1515 - accuracy: 0.9567 - 922ms/epoch - 8ms/step\n",
       "Calling model.evaluate()...\n",
       "20/20 - 2s - loss: 0.1042 - accuracy: 0.9664 - 2s/epoch - 85ms/step\n",
       "Evaluation results: [0.10418514907360077, 0.9664062261581421]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:5] TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:7850\", \"localhost:7851\", \"localhost:7852\", \"localhost:7853\", \"localhost:7854\", \"localhost:7855\", \"localhost:7856\", \"localhost:7857\"]}, \"task\": {\"type\": \"worker\", \"index\": 5}}\n",
       "   local_devices=('/job:worker/task:5/device:HPU:0',)\n",
       "total_batch_size = 64 * 8 workers = 512\n",
       "Loading MNIST dataset...\n",
       "Calling model.fit()...\n",
       "Epoch 1/2\n",
       "118/118 - 7s - loss: 0.4593 - accuracy: 0.8770 - 7s/epoch - 62ms/step\n",
       "Epoch 2/2\n",
       "118/118 - 1s - loss: 0.1515 - accuracy: 0.9567 - 921ms/epoch - 8ms/step\n",
       "Calling model.evaluate()...\n",
       "20/20 - 2s - loss: 0.1042 - accuracy: 0.9664 - 2s/epoch - 84ms/step\n",
       "Evaluation results: [0.10418514907360077, 0.9664062261581421]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:7850\", \"localhost:7851\", \"localhost:7852\", \"localhost:7853\", \"localhost:7854\", \"localhost:7855\", \"localhost:7856\", \"localhost:7857\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}\n",
       "   local_devices=('/job:worker/task:1/device:HPU:0',)\n",
       "total_batch_size = 64 * 8 workers = 512\n",
       "Loading MNIST dataset...\n",
       "Calling model.fit()...\n",
       "Epoch 1/2\n",
       "118/118 - 7s - loss: 0.4593 - accuracy: 0.8770 - 7s/epoch - 62ms/step\n",
       "Epoch 2/2\n",
       "118/118 - 1s - loss: 0.1515 - accuracy: 0.9567 - 922ms/epoch - 8ms/step\n",
       "Calling model.evaluate()...\n",
       "20/20 - 2s - loss: 0.1042 - accuracy: 0.9664 - 2s/epoch - 85ms/step\n",
       "Evaluation results: [0.10418514907360077, 0.9664062261581421]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:7850\", \"localhost:7851\", \"localhost:7852\", \"localhost:7853\", \"localhost:7854\", \"localhost:7855\", \"localhost:7856\", \"localhost:7857\"]}, \"task\": {\"type\": \"worker\", \"index\": 3}}\n",
       "   local_devices=('/job:worker/task:3/device:HPU:0',)\n",
       "total_batch_size = 64 * 8 workers = 512\n",
       "Loading MNIST dataset...\n",
       "Calling model.fit()...\n",
       "Epoch 1/2\n",
       "118/118 - 7s - loss: 0.4593 - accuracy: 0.8770 - 7s/epoch - 62ms/step\n",
       "Epoch 2/2\n",
       "118/118 - 1s - loss: 0.1515 - accuracy: 0.9567 - 921ms/epoch - 8ms/step\n",
       "Calling model.evaluate()...\n",
       "20/20 - 2s - loss: 0.1042 - accuracy: 0.9664 - 2s/epoch - 84ms/step\n",
       "Evaluation results: [0.10418514907360077, 0.9664062261581421]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:2] 2022-01-22 08:08:42.778554: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2022-01-22 08:08:44.557263: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.561464: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.571647: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:7850, 1 -> localhost:7851, 2 -> localhost:7852, 3 -> localhost:7853, 4 -> localhost:7854, 5 -> localhost:7855, 6 -> localhost:7856, 7 -> localhost:7857}\n",
       "2022-01-22 08:08:44.575023: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:7852\n",
       "2022-01-22 08:08:44.741001: I /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/synapse_helpers/hccl_communicator.cpp:55] Opening communication. Device id:0.\n",
       "2022-01-22 08:08:57.200107: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9743f0d31524416862daa9ec3c8b701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/8 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:4] 2022-01-22 08:08:42.777822: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2022-01-22 08:08:44.581317: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.587683: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.597617: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:7850, 1 -> localhost:7851, 2 -> localhost:7852, 3 -> localhost:7853, 4 -> localhost:7854, 5 -> localhost:7855, 6 -> localhost:7856, 7 -> localhost:7857}\n",
       "2022-01-22 08:08:44.601518: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:7854\n",
       "2022-01-22 08:08:44.739501: I /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/synapse_helpers/hccl_communicator.cpp:55] Opening communication. Device id:0.\n",
       "2022-01-22 08:08:57.200326: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] 2022-01-22 08:08:42.776619: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2022-01-22 08:08:44.605777: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.608621: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.618580: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:7850, 1 -> localhost:7851, 2 -> localhost:7852, 3 -> localhost:7853, 4 -> localhost:7854, 5 -> localhost:7855, 6 -> localhost:7856, 7 -> localhost:7857}\n",
       "2022-01-22 08:08:44.620449: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:7851\n",
       "2022-01-22 08:08:44.740687: I /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/synapse_helpers/hccl_communicator.cpp:55] Opening communication. Device id:0.\n",
       "2022-01-22 08:08:57.200290: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:5] 2022-01-22 08:08:42.777927: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2022-01-22 08:08:44.601381: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.606835: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.611995: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:7850, 1 -> localhost:7851, 2 -> localhost:7852, 3 -> localhost:7853, 4 -> localhost:7854, 5 -> localhost:7855, 6 -> localhost:7856, 7 -> localhost:7857}\n",
       "2022-01-22 08:08:44.614041: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:7855\n",
       "2022-01-22 08:08:44.740553: I /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/synapse_helpers/hccl_communicator.cpp:55] Opening communication. Device id:0.\n",
       "2022-01-22 08:08:57.199882: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:3] 2022-01-22 08:08:42.777006: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2022-01-22 08:08:44.615701: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.620038: W /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/habana_device/habana_device.cpp:182] HPU initialization done for library version 1.2.0_c6aea18b_tf2.7.0\n",
       "2022-01-22 08:08:44.627451: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:7850, 1 -> localhost:7851, 2 -> localhost:7852, 3 -> localhost:7853, 4 -> localhost:7854, 5 -> localhost:7855, 6 -> localhost:7856, 7 -> localhost:7857}\n",
       "2022-01-22 08:08:44.630222: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:7853\n",
       "2022-01-22 08:08:44.771511: I /home/jenkins/workspace/cdsoftwarebuilder/create-tensorflow-module---bpt-d/tensorflow-training/synapse_helpers/hccl_communicator.cpp:55] Opening communication. Device id:0.\n",
       "2022-01-22 08:08:57.200331: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "if __name__ == \"__main__\":\n",
    "    train_mnist(batch_size=64,num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df240956",
   "metadata": {},
   "source": [
    "## Training has been done! Remember to shutdown the mpi engines to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfda8a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controller stopped: {'exit_code': 0, 'pid': 9307, 'identifier': 'ipcontroller-1642838911-z6z6-9290'}\n",
      "engine set stopped 1642838912: {'exit_code': 0, 'pid': 9402, 'identifier': 'ipengine-1642838911-z6z6-1642838912-9290'}\n"
     ]
    }
   ],
   "source": [
    "client.shutdown(hub=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
