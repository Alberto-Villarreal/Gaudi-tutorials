{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56816687",
   "metadata": {},
   "source": [
    "# Data Packing Process for MLPERF BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2352b0f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7f6a0",
   "metadata": {},
   "source": [
    "Often NLP datasets have large variations in their samples length. Setting a maximum sequence length (max_seq_len) and pad shorter sequences with zeros is a common approach used with GPUs and CPUs.This approach is very inefficient as it results in many unrequired operations (multiply by zeros). The potential speed up by avoiding padding is the ratio of max_seq_len over average sequence length (avg_seq_len) in the NLP dataset. For MLPerf's BERT dataset the potential speedup is roughly 2. \n",
    "\n",
    "For Habana's V1.1 MLPerf submission, we used a data packing technique [1], called Non-Negative Least-Square histogram. Here instead of padding with zero, we pack several short sequences to one multi-sequence of size max_seq_len. Thus, we remove most of the padding, which can lead to up to x2 speedup in MLPerf BERT benchmark time-to-train (TTT). This packing technique can be applied on other types of datasets with high variability in samples length.\n",
    "\n",
    "Please note that for each dataset with sequential data samples, the specific speedup with data packing is determined by the ratio of max_seq_len to average_seq_len in that particular NLP dataset. The larger the ratio, the higher the speedup. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546ef55",
   "metadata": {},
   "source": [
    "![title](img_jupyter/before_after_packing.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f18e58",
   "metadata": {},
   "source": [
    "## Packing algorithm - Non-Negative Least-Square histogram:\n",
    "(1) Go over the entire dataset and extract a histogram, $H$, of the sequence lengths\n",
    "\n",
    "(2) Define a maximum number of sequences in a pack (we used 3) and find all possible combinations of sequence lengths that sum to max_seq_length. Those are all the possible strategies to pack. \n",
    "\n",
    "(3) Create a matrix A of size $\\textrm{max_seq_len} \\times \\textrm{num_possible stratagies}$, where:\n",
    "\\begin{equation}\n",
    "A_{i,j}=\n",
    "\\begin{cases}\n",
    "    1 ,& \\text{if } \\textrm{sequence} ~i~ \\textrm{is part of strategy} ~j\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "(4) Solve iterativly:\n",
    "\\begin{equation}\n",
    "            min_x|Ax=H|_2 \\\\\n",
    "           \n",
    "            s.t \\hspace{5pt} x>=0. \n",
    "\\end{equation}\n",
    "More technical details are in the code below and the paper [1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42c812",
   "metadata": {},
   "source": [
    "## Data format\n",
    "When packing the data addtional fields should be added to the samples format to ensure mathematically equivlant operations. Below we detail the format changes.\n",
    "\n",
    "### Data format before packing:\n",
    "- **input_ids**: size=512, list of token ids, where:\n",
    "    - 101 - start \n",
    "    - 102 - end\n",
    "    - 103 - mask  <br>\n",
    "    padded to 512 with zeros.\n",
    "- **input_mask**: size=512, 111...111000...000, where the number of ones corresponds to the effective sample length, padded to 512 with zeros.\n",
    "- **segment_ids**: size=512, 000...000111...111000...000, where first zeros correspond to the first sentence, ones to second sentence, and  padded to 512 with zeros.\n",
    "- **masked_lm_positions**: size=76, positions of masked tokens (103),  padded to 76 with zeros.\n",
    "- **masked_lm_ids**: size=76, token ids of masked tokens,  padded to 76 with zeros.\n",
    "- **masked_lm_weights**: size=76, 111...111000...000, number of ones equals to number of masked tokens.\n",
    "- **next_sentence_labels**: size=1, 0 or 1, where 1 if sentence 2 is the next sentence of sentence 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4f8fb",
   "metadata": {},
   "source": [
    "### Data format after packing:\n",
    "- **input_ids**: size=512, list of token ids where: \n",
    "    - 101 - start \n",
    "    - 102 - end\n",
    "    - 103 - mask  <br>\n",
    "    padded to 512 with zeros. <br>\n",
    "    Example of 2 packed samples: 101,...,102,...,102,101,...,102,...,102,0...0 <br>\n",
    "    where 101,...,102 first sentence, ,...,102 second sentence, <br>\n",
    "    101,...,102 third sentence and ,...,102 forth sentence.\n",
    "- **input_mask**: size=512, 111...111222...222000...000, \n",
    "where the number of ones corresponds to the first sample length, \n",
    "and the number of twos corresponds to the second sample length. (If there are 3 samples 1...12...23...30...0.)\n",
    "- **segment_ids**: size=512, 000...000111...111000...000111...111000...000 \n",
    "where 000...000111...111 the first and the second samples, padded to 512 with zeros.\n",
    "- **positions**: size=512, 0,1,2,3,...,\\<length of first sample\\> - 1,0,1,2,3,...,\\<length of second sample\\> - 1,0,0,...,0\n",
    "- **masked_lm_positions**: size=79, positions of masked tokens (103), padded to 79 with zeros.\n",
    "- **masked_lm_ids**: size=79, token ids of masked tokens, padded to 79 with zeros.\n",
    "- **masked_lm_weights**: size=79, 111...111222...222000...000 \n",
    "where 111...111 corresponds to the first sample\n",
    "and 222...222 to the second. (If there are 3 samples 1...12...23...30...0.)\n",
    "- **next_sentence_positions**: size=3, \\<position of first sample\\>,\\<position of second sample\\>,0 (corresponds to 101 positions)\n",
    "- **next_sentence_labels**: size=3, 0 or 1, where 1 if sentence 2 is the next sentence of sentence 1.\n",
    "- **next_sentence_weights**: size=3, 110 (If there are 3 samples 111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fade0a",
   "metadata": {},
   "source": [
    "## Required Modifications to The Model\n",
    "\n",
    "In addition to packing the data, minor changes should be applied to the model.\n",
    "\n",
    "<a id=’position_embedding’></a> \n",
    "#### (1) Position embedding: <br> \n",
    "Position embedding is a matrix of the shape `(512, 1024)`, where row `i` in this matrix corresponds to the token at position `i`\n",
    "in the sample. Before packing, all positions were in increasing order, and thus we could slice first `n` rows from the position\n",
    "embedding matrix. After packing, token positions are no longer increasing sequence. Instead (in the case of 3 packed samples)\n",
    "it is a stack of three increasing sequences (look at **positions** field). Consequently, instead of using a slice, we should select\n",
    "rows in the position embedding matrix according to **positions** field.\n",
    "<a id=’attention_mask’></a> \n",
    "#### (2) Attention mask: <br>\n",
    "Before packing, an attention mask was used by attention layers to filter out all paddings. For example, if we had a sample with an effective length of\n",
    "`n <= 512`, in **input_ids** field we would have `n` token ids padded by zeros till `512`. In this case, the attention mask would\n",
    "be looked like in the following image: <br>\n",
    "![title](img_jupyter/attention_mask_before_packing.png)\n",
    "i.e., matrix of the shape `(512, 512)` with the block of ones of shape `(n, n)` and all other elements are zeros. <br>\n",
    "After packing, in addition to padding, we should filter out all cross-sample attentions, i.e., weight between tokens of two different samples And thus, for the case of 3 packed samples with the lengths `n`, `m` and `k` the \n",
    "attention matrix should be looked like in the following image: <br>\n",
    "![title](img_jupyter/attention_mask_after_packing.png)\n",
    "i.e. matrix of the shape `(512, 512)` with blocks of ones of shapes `(n, n)`, `(m, m)` and `(k, k)`, and all other elements\n",
    "are zeros. We should use **input_mask** field to build such an attention mask matrix.\n",
    "\n",
    "<a id=’pooler’></a> \n",
    "#### (3) Pooler: <br>\n",
    "The output matrix of the model is a tensor of shape `(512, 1024)` (512 tokens and 1024 embedding dimension) i.e. 512 embedded tokens, where the first embedded token corresponds to 101 id. Before packing, only the first embedded token (101) was selected \n",
    "for the next sentence prediction task (the task where we should predict the sample's following sentence), and thous we selected the first row in output matrix. After packing, we have up to 3 packed samples (where each \n",
    "multi-sample can be a pair of  consecutive sentences), and thus, for each sample, we have also the position of  its 101 token, which should \n",
    "be taken for the next sentence prediction task, therefore we should use **next_sentence_positions** field to select the correct embedded \n",
    "token matrix.\n",
    "\n",
    "<a id=’next_sentence_loss’></a> \n",
    "#### (4) Next sentence loss: <br>\n",
    "Before packing, there was only one pair of maybe consecutive sentences in each sample, thus we computed loss for each sample\n",
    "and averaged upon batch size. After packing, each multi-sample contains a different number of packed samples, from one to three,\n",
    "so we should average accross the effective number of samples in batch size. For that we should use **next_sentence_weights** field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd665ba",
   "metadata": {},
   "source": [
    "# Examples\n",
    "Next we dive into more detailed examples of the required model changes and the packing process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fe726",
   "metadata": {},
   "source": [
    "## Example 1: modifing the MLPerf's BERT model\n",
    "\n",
    "Specifically For MLPerf's BERT model the following functions were modified:\n",
    "\n",
    "### 1) In run_pretraining.py script input_fn_builder function: Since after packing we have different fields, we should change input_fn_builder.<br>\n",
    "\n",
    "   - $\\color{teal}{\\text{Unpacked mode}}$:\n",
    "    \n",
    "    name_to_features = {\n",
    "              \"input_ids\":\n",
    "                  tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "              \"input_mask\":\n",
    "                  tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "              \"segment_ids\":\n",
    "                  tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "              \"masked_lm_positions\":\n",
    "                  tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "              \"masked_lm_ids\":\n",
    "                  tf.io.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "              \"masked_lm_weights\":\n",
    "                  tf.io.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "              \"next_sentence_labels\":\n",
    "                  tf.io.FixedLenFeature([1], tf.int64),\n",
    "          }\n",
    "    \n",
    "   - $\\color{teal}{\\text{Packed mode}}$:\n",
    "    \n",
    "    \n",
    "    name_to_features = {\n",
    "            \"input_ids\":\n",
    "              tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"input_mask\":\n",
    "              tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"segment_ids\":\n",
    "              tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"positions\":\n",
    "              tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"masked_lm_positions\":\n",
    "              tf.io.FixedLenFeature([max_predictions_per_seq + 3], tf.int64),\n",
    "            \"masked_lm_ids\":\n",
    "              tf.io.FixedLenFeature([max_predictions_per_seq + 3], tf.int64),\n",
    "            \"masked_lm_weights\":\n",
    "              tf.io.FixedLenFeature([max_predictions_per_seq + 3], tf.float32),\n",
    "            \"next_sentence_positions\":\n",
    "              tf.io.FixedLenFeature([3], tf.int64),\n",
    "            \"next_sentence_labels\":\n",
    "              tf.io.FixedLenFeature([3], tf.int64),\n",
    "            \"next_sentence_weights\":\n",
    "              tf.io.FixedLenFeature([3], tf.float32),\n",
    "          }\n",
    "    \n",
    "   \n",
    "### 2) In run_pretraining.py script model_fn_builder function: <br>\n",
    "    \n",
    "   - $\\color{teal}{\\text{Unpacked mode}}$:\n",
    "    \n",
    "   \n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "    masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "    masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "    next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "   \n",
    "   - $\\color{teal}{\\text{Packed mode}}$:\n",
    "    \n",
    "    \n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "    masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "    masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "    next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "    \n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    positions = None\n",
    "    next_sentence_positions = None\n",
    "    next_sentence_weights = None\n",
    "    if FLAGS.enable_packed_data_mode and is_training: # only training should work in packed mode\n",
    "        positions = features[\"positions\"]\n",
    "        next_sentence_positions = features[\"next_sentence_positions\"]\n",
    "        next_sentence_weights = features[\"next_sentence_weights\"]\n",
    "    \n",
    "\n",
    "### 3) In modeling.py script embedding_postprocessor function: Added position embedding selection according to subsection in [1](#position_embedding) **Model changes** part.\n",
    "   - $\\color{teal}{\\text{For packed model the following code was added}}$:\n",
    "    \n",
    "    \n",
    "    flat_positions = tf.reshape(positions, [-1])\n",
    "    if use_one_hot_embeddings:\n",
    "        one_hot_positions = tf.one_hot(flat_positions, depth=seq_length)\n",
    "        position_embeddings = tf.matmul(one_hot_positions, position_embeddings)\n",
    "    else:\n",
    "        position_embeddings = tf.gather(position_embeddings, flat_positions)\n",
    "        position_embeddings = tf.reshape(position_embeddings, [batch_size, seq_length, width])\n",
    "    \n",
    "\n",
    "### 4) In modeling.py script create_attention_mask_from_input_mask function: Built block matrix according subsection [2](#attention_mask) in **Model changes** part. \n",
    "   - $\\color{teal}{\\text{Unpacked mode}}$:\n",
    "    \n",
    "    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "    batch_size = from_shape[0]\n",
    "    \n",
    "    to_shape = get_shape_list(to_mask, expected_rank=2)\n",
    "    to_seq_length = to_shape[1]\n",
    "    \n",
    "    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
    "    \n",
    "    \n",
    "   - $\\color{teal}{\\text{Packed mode}}$:\n",
    "    \n",
    "    to_mask = tf.one_hot(to_mask - 1, depth=3)\n",
    "    to_mask = tf.matmul(to_mask, to_mask, transpose_b=True)\n",
    "    \n",
    "\n",
    "### 5) In modeling.py script BertModel.\\__init\\__  function pooler part: Selected first embedded token (101) for each packed sample according subsection  [3](#pooler) in **Model changes** part.\n",
    "   - $\\color{teal}{\\text{Unpacked mode}}$:\n",
    "    \n",
    "    selected_tokens = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "    \n",
    "    \n",
    "   - $\\color{teal}{\\text{Packed mode}}$:\n",
    "    \n",
    "    selected_tokens = gather_indexes(self.sequence_output, next_sentence_positions)\n",
    "    \n",
    "\n",
    "### 6) In run_pretraining.py script get_masked_lm_output function: Added float32 casting of **masked_lm_weights** field.\n",
    "   - $\\color{teal}{\\text{Packed mode}}$:\n",
    "    \n",
    "    label_weights = tf.cast(label_weights > 0, dtype=tf.float32)\n",
    "    \n",
    "\n",
    "### 7) In run_pretraining.py script get_next_sentence_output function: Applied change according to subsection [4](#next_sentence_loss)  in **Model changes** part.\n",
    "   - $\\color{teal}{\\text{Unpacked mode}}$:\n",
    "    \n",
    "    loss = tf.reduce_mean(input_tensor=per_example_loss)\n",
    "    \n",
    "    \n",
    "   - $\\color{teal}{\\text{Packed mode}}$:\n",
    "    \n",
    "    weights = tf.reshape(weights, [-1])\n",
    "    numerator = tf.reduce_sum(input_tensor=weights * per_example_loss)\n",
    "    denominator = tf.reduce_sum(input_tensor=weights) + 1e-5\n",
    "    loss = numerator / denominator\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992057b",
   "metadata": {},
   "source": [
    "## Example 2 :  packing the TF records of a toy datatset\n",
    "\n",
    "Next, we demonstrate how we addopted the code suggested by [1] and convert it to packed TF records. In the example below, we pack a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3641951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import struct\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "from itertools import repeat, chain\n",
    "from functools import lru_cache, reduce\n",
    "from collections import defaultdict, OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67ee354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "        input_glob=\"/root/tensorflow_datasets/MLPerf_BERT_Wiki/unpacked_toy_data/\"\n",
    "        output_dir=\"/root/tensorflow_datasets/MLPerf_BERT_Wiki/packed_toy_data/\"\n",
    "        random_seed=12345\n",
    "        max_files=6  #default 100\n",
    "        duplication_factor=1\n",
    "        max_sequence_length=512\n",
    "        max_predictions_per_sequence=76\n",
    "        max_sequences_per_pack=3\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d9d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def packing_strategies(start, previous, target, depth):\n",
    "    gap = target - start\n",
    "\n",
    "    # The collection of possible strategies given the\n",
    "    # starting sum, the target sum, and the available depth\n",
    "    # strategy search is limited to increments greater or equal to previous\n",
    "    strategies = []\n",
    "    # Complete the packing with exactly 1 number\n",
    "    if depth == 1:\n",
    "        if gap >= previous:\n",
    "            strategies.append([gap])\n",
    "\n",
    "    # Complete the sample in \"depth\" steps, recursively\n",
    "    else:\n",
    "        for new in range(previous, gap + 1):\n",
    "\n",
    "            new_gap = target - start - new\n",
    "            if new_gap == 0:\n",
    "                strategies.append([new])\n",
    "            else:\n",
    "                options = packing_strategies(start + new, new, target, depth - 1)\n",
    "\n",
    "                for option in options:\n",
    "                    if len(option) > 0:\n",
    "                        strategies.append([new] + option)\n",
    "    return strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e041a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_packing_recipe(sequence_lengths, max_sequence_length, max_sequences_per_pack=3):\n",
    "    # Histogram of sequence lengths\n",
    "    histogram, bins = np.histogram(sequence_lengths, bins=np.arange(1, max_sequence_length + 2))\n",
    "    print(\"Begin packing pass\".center(80, \"_\"))\n",
    "    print(f\"Unpacked mean sequence length: {sequence_lengths.mean():3.2f}\")\n",
    "    \n",
    "    # Make sure all strategies are recipes to pack to the correct sequence length\n",
    "    strategy_set = packing_strategies(0, 1, max_sequence_length, max_sequences_per_pack)\n",
    "    for strategy in strategy_set:\n",
    "        assert(sum(strategy) == max_sequence_length)\n",
    "    num_strategies = len(strategy_set)\n",
    "    print(f\"Found {num_strategies} unique packing strategies.\")\n",
    "\n",
    "    # Solve the packing equation A@mixture = histogram\n",
    "    A = np.zeros((max_sequence_length, num_strategies), dtype=np.int32)\n",
    "    for i in range(num_strategies):\n",
    "        strategy = strategy_set[i]\n",
    "        for seq_len in strategy:\n",
    "            A[seq_len - 1, i] += 1\n",
    "\n",
    "    # short sequences are inexpensive to add, so should have low residual weights\n",
    "    # to exactly minimize padding use w0 = np.arange(1, max_sequence_length + 1)\n",
    "    # in practice the difference is negligible, but this converges faster\n",
    "    padding_cutoff = 8\n",
    "    w0 = np.ones([max_sequence_length])\n",
    "    # w0 = np.linspace(1, max_sequence_length+1, max_sequence_length)/max_sequence_length  # padding minimization weight\n",
    "    w0[:padding_cutoff] = padding_cutoff / (2 * max_sequence_length)\n",
    "    w0 = np.sqrt(w0)\n",
    "\n",
    "    # Starting values for the padding and the mixture\n",
    "    padding = np.zeros([max_sequence_length], dtype=np.int32)\n",
    "    mixture = np.zeros([num_strategies], dtype=np.int32)\n",
    "    b = histogram + padding\n",
    "\n",
    "    # Pack sequences as best as possible, then increase padding accordingly and repeat\n",
    "    for i in range(0, 20):\n",
    "        print(f\"\\nIteration: {i}: sequences still to pack: \", b.sum())\n",
    "        start = time.time()\n",
    "        partial_mixture, rnorm = optimize.nnls(np.expand_dims(w0, -1) * A, w0 * b)\n",
    "        print(f\"Solving nnls took {time.time() - start:3.2f} seconds.\")\n",
    "        print(f\"Residual norm:  {rnorm:3.5e}\")\n",
    "\n",
    "        # Update mixture (round the floating point solution to integers)\n",
    "        partial_mixture = np.where(partial_mixture < 2, np.rint(partial_mixture), np.floor(partial_mixture))\n",
    "\n",
    "        # If partial mixture is empty (due to rounding) we follow the gradient\n",
    "        # this usually happens when the number of examples is small i.e. ~100\n",
    "        if partial_mixture.max() == 0:\n",
    "            grad = A.T @ (b * np.arange(1, max_sequence_length + 1))\n",
    "            k = int(b.sum() // 2) + 1\n",
    "            topk = np.argsort(-grad)[:k]\n",
    "            partial_mixture[topk] += 1\n",
    "\n",
    "        # Update mixture\n",
    "        mixture = mixture + partial_mixture\n",
    "\n",
    "        # Compute the residuals\n",
    "        residual = b - A @ partial_mixture\n",
    "        print(f\"Max residual:   {abs(residual).max()}\")\n",
    "        print(f\"Residual on first 8 categories: {np.around(residual[:8], 4)}\")\n",
    "        print(f\"Residual on last 8 categories:  {np.around(residual[-8:], 4)}\")\n",
    "\n",
    "        # Add padding based on deficit (negative residual)\n",
    "        partial_padding = np.where(residual < 0, -residual, 0)\n",
    "        print(f\"Added {(partial_padding*np.arange(1,max_sequence_length+1)).sum():3.2e} tokens of padding.\")\n",
    "        padding = padding + partial_padding\n",
    "\n",
    "        # Update the rhs vector (remaining surplus sequences)\n",
    "        b = histogram + padding - A @ mixture\n",
    "        assert np.all(b >= 0), b\n",
    "\n",
    "        # Done iterating\n",
    "        if b.sum() < 100:\n",
    "            break\n",
    "\n",
    "    # Make sure there is no remainder\n",
    "    unpacked_seqlen = np.arange(1, args.max_sequence_length + 1)[b > 0]\n",
    "    # Update the mixture to also covered the unpacked sequences\n",
    "    for l in unpacked_seqlen:\n",
    "        # Get the depth 1 strategy\n",
    "        strategy = sorted([l, args.max_sequence_length - l])\n",
    "        strategy_index = strategy_set.index(strategy)\n",
    "        mixture[strategy_index] += b[l-1]\n",
    "    b = histogram - A @ mixture\n",
    "    padding = np.where(b < 0, -b, 0)\n",
    "    b = histogram + padding - A @ mixture\n",
    "    assert b.sum() == 0\n",
    "\n",
    "    # Analyze result\n",
    "    print(\"Done solving for packing order\".center(80, \"_\"))\n",
    "    num_padding_tokens = (np.arange(1, max_sequence_length + 1) * padding).sum()\n",
    "    num_padding_tokens_original = (max_sequence_length - sequence_lengths).sum()\n",
    "    print(f\"Number of sequences dropped:  {b.sum()}\")\n",
    "    print(f\"Number of strategies utilized: {np.count_nonzero(mixture)}\")\n",
    "    new_number_of_samples = int(mixture.sum())\n",
    "    compression = 1 - new_number_of_samples / len(sequence_lengths)\n",
    "    print(f\"New number of samples: {new_number_of_samples:3.2f}, original {len(sequence_lengths)}. A compression ratio of {compression:3.3f}\")\n",
    "    print(f\"The expected speed-up from packing: {1/(1-compression):3.3f}\")\n",
    "    upper_bound = 1.0 / (1 - ((1 - sequence_lengths / max_sequence_length).mean()))\n",
    "    print(f\"Theoretical upper bound on speed-up: {upper_bound:3.3f}\")\n",
    "    avg_sequences_per_sample = ((A.sum(0) * mixture).sum() - padding.sum()) / new_number_of_samples\n",
    "    print(f\"Average sequences/sample {avg_sequences_per_sample:3.5f}\")\n",
    "    print(f\"Added {num_padding_tokens:3.2e} padding tokens. Original dataset used {num_padding_tokens_original:3.2e} padding tokens\")\n",
    "    efficiency = (new_number_of_samples*max_sequence_length - num_padding_tokens)/(new_number_of_samples*max_sequence_length)\n",
    "    print(f\"Packing efficiency (fraction of real tokens): {efficiency:3.4f}\")\n",
    "\n",
    "    print(f\"Top 8 strategies\")\n",
    "    topK = np.argsort(-mixture)[:8]\n",
    "    for i in topK:\n",
    "        print(f\"Strategy {strategy_set[i]} which is used {int(mixture[i])} times\")\n",
    "    print(\"\".center(80, \"_\"))\n",
    "\n",
    "    # Figure out the slicing that each strategy should use\n",
    "    slicing = np.zeros_like(A)\n",
    "    slicing[:, 1:] = np.cumsum(A * mixture, axis=1)[:, :-1]\n",
    "    slicing = slicing.T\n",
    "\n",
    "    mixture = mixture.astype(np.int64)    \n",
    "    return strategy_set, mixture, padding, slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c373be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_examples(examples_by_length, slicing, strategy_set, repeat_counts):\n",
    "    # Divide the work, firstly between the strategies and then into chunks of 50k\n",
    "    slices = []\n",
    "    strategies = []\n",
    "    part_idx = []\n",
    "    for strategy, slice_offsets, repeat_count in zip(strategy_set, slicing, repeat_counts):\n",
    "        if repeat_count == 0:\n",
    "            continue\n",
    "        # Slice out the sequences allocated to this strategy in increments of 50k\n",
    "        num_parts = repeat_count // 50000\n",
    "        num_parts = num_parts + int(repeat_count != num_parts * 50000)\n",
    "        subcounts = (min(50000, repeat_count - 50000 * (i - 1)) for i in range(1, num_parts + 1))\n",
    "        for part_id, part_count in enumerate(subcounts):\n",
    "            examples = []\n",
    "            for k, seq_len in enumerate(strategy):\n",
    "                slice_start = int(slice_offsets[seq_len - 1])\n",
    "                slice_end = slice_start + int(part_count)\n",
    "                slice_offsets[seq_len - 1] = slice_end\n",
    "                examples.append(examples_by_length[seq_len][slice_start:slice_end])\n",
    "            #import pdb; pdb.set_trace()\n",
    "            slices.append(examples)\n",
    "            strategies.append(strategy)\n",
    "            part_idx.append(part_id)\n",
    "\n",
    "    return slices, strategies, part_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2581a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_pack_according_to_strategy(args, part_idx, strategy, examples):\n",
    "    # Pack the sequences according to the strategy and write them to disk\n",
    "    base_filename = os.path.join(args.output_dir, \"strategy_\" + \"_\".join(map(str, strategy)))\n",
    "    filename = base_filename + f\"_part_{part_idx}\"\n",
    "    writer = tf.compat.v1.python_io.TFRecordWriter(filename)\n",
    "    for i, multi_sequence in enumerate(zip(*examples)):\n",
    "        features = create_multi_sequence_example(multi_sequence, args.max_predictions_per_sequence,\n",
    "                                                       args.max_sequence_length, args.max_sequences_per_pack)\n",
    "    # Write to file\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12079ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_sequence_example(multi_sequence, max_predictions_per_sequence, max_sequence_length, max_sequences_per_pack):\n",
    "    # SEQ\n",
    "    packed_input_ids = np.zeros(max_sequence_length, dtype=np.int32)\n",
    "    packed_input_mask = np.zeros(max_sequence_length, dtype=np.int32)\n",
    "    packed_segment_ids = np.zeros(max_sequence_length, dtype=np.int32)\n",
    "    packed_positions = np.zeros(max_sequence_length, dtype=np.int32)\n",
    "\n",
    "    # MLM\n",
    "    # we are packing up to max_sequences_per_pack, each with a certain percentage of masked tokens\n",
    "    # in case that percentege is rounded up for all sequences in the pack, need to add an extra token for\n",
    "    # each sequence in the pack\n",
    "    packed_masked_lm_positions = np.zeros(max_predictions_per_sequence + max_sequences_per_pack, dtype=np.int32)\n",
    "    packed_masked_lm_ids = np.zeros(max_predictions_per_sequence + max_sequences_per_pack, dtype=np.int32)\n",
    "    packed_masked_lm_weights = np.zeros(max_predictions_per_sequence + max_sequences_per_pack, dtype=np.int32)\n",
    "\n",
    "    # NSP\n",
    "    packed_next_sentence_positions = np.zeros(max_sequences_per_pack, dtype=np.int32)\n",
    "    packed_next_sentence_labels = np.zeros(max_sequences_per_pack, dtype=np.int32)\n",
    "    packed_next_sentence_weights = np.zeros(max_sequences_per_pack, dtype=np.int32)\n",
    "\n",
    "    offset = 0\n",
    "    mlm_offset = 0\n",
    "    sequence_index = 1  # used in the input mask\n",
    "    for sequence in multi_sequence:\n",
    "        # Padding sequences are donoted with None\n",
    "        if sequence is not None:\n",
    "            example = tf.train.Example()\n",
    "            example.ParseFromString(sequence.numpy())\n",
    "\n",
    "            input_ids = np.array(example.features.feature['input_ids'].int64_list.value)\n",
    "            input_mask = np.array(example.features.feature['input_mask'].int64_list.value)\n",
    "            segment_ids = np.array(example.features.feature['segment_ids'].int64_list.value)\n",
    "            masked_lm_positions = np.array(example.features.feature['masked_lm_positions'].int64_list.value)\n",
    "            masked_lm_ids = np.array(example.features.feature['masked_lm_ids'].int64_list.value)\n",
    "            masked_lm_weights = np.array(example.features.feature['masked_lm_weights'].float_list.value)\n",
    "            next_sentence_labels = np.array(example.features.feature['next_sentence_labels'].int64_list.value)\n",
    "\n",
    "            #input_ids, input_mask, segment_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights, next_sentence_labels = sequence\n",
    "            seq_len = input_mask.sum()\n",
    "\n",
    "            # SEQ\n",
    "            packed_input_ids[offset:offset + seq_len] = input_ids[:seq_len]\n",
    "            packed_input_mask[offset:offset + seq_len] = sequence_index\n",
    "            packed_segment_ids[offset:offset + seq_len] = segment_ids[:seq_len]\n",
    "            packed_positions[offset:offset + seq_len] = np.arange(0, seq_len)\n",
    "\n",
    "            # MLM\n",
    "            mlm_len = int(masked_lm_weights.sum())\n",
    "            assert mlm_offset + mlm_len < max_predictions_per_sequence + max_sequences_per_pack, \"Too many LM predictions per sequences\"\n",
    "            max_mlm = mlm_offset + mlm_len\n",
    "            #import pdb; pdb.set_trace()\n",
    "            packed_masked_lm_positions[mlm_offset:max_mlm] = offset + masked_lm_positions[:mlm_len]\n",
    "            packed_masked_lm_ids[mlm_offset:max_mlm] = masked_lm_ids[:mlm_len]\n",
    "            packed_masked_lm_weights[mlm_offset:max_mlm] = sequence_index\n",
    "            #import pdb; pdb.set_trace()\n",
    "            # NSP\n",
    "            packed_next_sentence_positions[sequence_index - 1] = offset\n",
    "            packed_next_sentence_labels[sequence_index - 1] = next_sentence_labels\n",
    "            packed_next_sentence_weights[sequence_index - 1] = 1\n",
    "\n",
    "            # Update offsets\n",
    "            sequence_index += 1\n",
    "            offset += seq_len\n",
    "            mlm_offset = max_mlm\n",
    "            #import pdb; pdb.set_trace()\n",
    "    # Pack into tfrecord format:\n",
    "    \n",
    "    features = OrderedDict()\n",
    "    \n",
    "    features[\"input_ids\"] = create_int_feature(packed_input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(packed_input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(packed_segment_ids)\n",
    "    features[\"positions\"] = create_int_feature(packed_positions)\n",
    "    features[\"masked_lm_positions\"] = create_int_feature(packed_masked_lm_positions)\n",
    "    features[\"masked_lm_ids\"] = create_int_feature(packed_masked_lm_ids)\n",
    "    features[\"masked_lm_weights\"] = create_float_feature(packed_masked_lm_weights)    \n",
    "    features[\"next_sentence_positions\"] = create_int_feature(packed_next_sentence_positions)\n",
    "    features[\"next_sentence_labels\"] = create_int_feature(packed_next_sentence_labels)\n",
    "    features[\"next_sentence_weights\"] = create_float_feature(packed_next_sentence_weights)    \n",
    "    return features   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d21f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c59ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_float_feature(values):\n",
    "  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c460af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path=\"/root/tensorflow_datasets/MLPerf_BERT_Wiki/unpacked_toy_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c2c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out=\"/root/tensorflow_datasets/MLPerf_BERT_Wiki/packed_toy_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3784fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    \n",
    "    random.seed(args.random_seed)\n",
    "\n",
    "    # Put examples into bins depending on their sequence lengths and extract the sequence length\n",
    "    sequence_lengths = []\n",
    "    examples_by_length = defaultdict(list)\n",
    "    print(\"Looping through dataset to collect sequence length information...\")\n",
    "    for filename in os.listdir(args.input_glob):\n",
    "        for record in tf.data.TFRecordDataset(args.input_glob+filename):\n",
    "                example = tf.train.Example()\n",
    "                example.ParseFromString(record.numpy())\n",
    "                im_length = sum(example.features.feature['input_mask'].int64_list.value)\n",
    "                examples_by_length[im_length].append(record)\n",
    "                sequence_lengths.append(im_length)\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "\n",
    "    # Pass the array of sequence lengths to the packing algorithm\n",
    "    \n",
    "    strategy_set, mixture, padding, slicing = get_packing_recipe(sequence_lengths, args.max_sequence_length, args.max_sequences_per_pack)\n",
    "    \n",
    "    # Add the calculated padding\n",
    "    for i in range(1, args.max_sequence_length + 1):\n",
    "        examples_by_length[i].extend([None] * int(padding[i - 1]))\n",
    "\n",
    "    # Shuffle the data\n",
    "    for key in examples_by_length:\n",
    "        random.shuffle(examples_by_length[key])\n",
    "\n",
    "    # Pack and store the data\n",
    "    print(f\"\\nPacking and writing packed dataset to {args.output_dir}.\")\n",
    "\n",
    "    # Slice the data into chunks of max 50k packed examples\n",
    "    example_slices, strategies, part_idx = slice_examples(examples_by_length, slicing, strategy_set, mixture)\n",
    "    print(f\"Splitting work into {len(part_idx)} parts.\")\n",
    "    start = time.time()\n",
    "    #For debug uses\n",
    "    #for i in range(len(part_idx)):\n",
    "    #    parallel_pack_according_to_strategy(args, part_idx[i], strategies[i], example_slices[i])\n",
    "    #import pdb; pdb.set_trace()\n",
    "\n",
    "    with ProcessPoolExecutor(16) as executor:\n",
    "        work = repeat(args), part_idx, strategies, example_slices\n",
    "        for partial_result in executor.map(parallel_pack_according_to_strategy, *work):\n",
    "            pass\n",
    "    print(f\"\\nDone. Took: {time.time() - start:3.2f} seconds to pack and write dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "180002b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through dataset to collect sequence length information...\n",
      "_______________________________Begin packing pass_______________________________\n",
      "Unpacked mean sequence length: 397.15\n",
      "Found 22102 unique packing strategies.\n",
      "\n",
      "Iteration: 0: sequences still to pack:  191942\n",
      "Solving nnls took 24.56 seconds.\n",
      "Residual norm:  1.04327e+02\n",
      "Max residual:   495.0\n",
      "Residual on first 8 categories: [ -93. -149. -206. -265. -321. -378. -436. -495.]\n",
      "Residual on last 8 categories:  [4. 3. 3. 3. 2. 2. 1. 0.]\n",
      "Added 1.68e+04 tokens of padding.\n",
      "\n",
      "Iteration: 1: sequences still to pack:  727.0\n",
      "Solving nnls took 16.21 seconds.\n",
      "Residual norm:  2.80530e+01\n",
      "Max residual:   37.0\n",
      "Residual on first 8 categories: [ -5.  -9. -13. -18. -23. -27. -33. -37.]\n",
      "Residual on last 8 categories:  [1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "Added 4.85e+03 tokens of padding.\n",
      "\n",
      "Iteration: 2: sequences still to pack:  136.0\n",
      "Solving nnls took 12.29 seconds.\n",
      "Residual norm:  1.26215e+01\n",
      "Max residual:   11.0\n",
      "Residual on first 8 categories: [ -1.  -2.  -4.  -4.  -6.  -8. -10. -11.]\n",
      "Residual on last 8 categories:  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Added 1.13e+03 tokens of padding.\n",
      "_________________________Done solving for packing order_________________________\n",
      "Number of sequences dropped:  0.0\n",
      "Number of strategies utilized: 638\n",
      "New number of samples: 148952.00, original 191942. A compression ratio of 0.224\n",
      "The expected speed-up from packing: 1.289\n",
      "Theoretical upper bound on speed-up: 1.289\n",
      "Average sequences/sample 1.28862\n",
      "Added 3.36e+04 padding tokens. Original dataset used 2.20e+07 padding tokens\n",
      "Packing efficiency (fraction of real tokens): 0.9996\n",
      "Top 8 strategies\n",
      "Strategy [512] which is used 116668 times\n",
      "Strategy [165, 347] which is used 142 times\n",
      "Strategy [104, 109, 299] which is used 140 times\n",
      "Strategy [158, 354] which is used 140 times\n",
      "Strategy [59, 453] which is used 139 times\n",
      "Strategy [205, 307] which is used 137 times\n",
      "Strategy [255, 257] which is used 136 times\n",
      "Strategy [188, 324] which is used 133 times\n",
      "________________________________________________________________________________\n",
      "\n",
      "Packing and writing packed dataset to /root/tensorflow_datasets/MLPerf_BERT_Wiki/packed_toy_data/.\n",
      "Splitting work into 640 parts.\n",
      "\n",
      "Done. Took: 55.98 seconds to pack and write dataset.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b8fcd-5315-4b8d-bebe-c154da969729",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[1]https://towardsdatascience.com/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing-eadb749962b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c13cad-848c-42a4-bac7-d905db8eeae4",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Habana Labs, Ltd. an Intel Company.<br>\n",
    "All rights reserved.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the “License”);\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
